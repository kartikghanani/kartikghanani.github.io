[
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Below is a list of references and resources used in the DisasterLens project:\n\n\n\n\nR\nLibraries: ggplot2, plotly, dplyr, tidyr, rpart, rpart.plot, car, caret, ggridges ,reshape2.\nQuarto\nUsed for creating this website.\nCSS Frameworks\nCustom CSS: styles.css\n\n\n\n\n\n\nEmergency Events Database (EM-DAT)\nSource: https://www.emdat.be/\nDescription: Comprehensive historical data on disasters worldwide."
  },
  {
    "objectID": "references.html#research-papers",
    "href": "references.html#research-papers",
    "title": "References",
    "section": "",
    "text": "[Title of Paper 1]\nAuthor(s): [Author Names]\nPublished: [Year]\nLink: [URL or DOI]\n[Title of Paper 2]\nAuthor(s): [Author Names]\nPublished: [Year]\nLink: [URL or DOI]"
  },
  {
    "objectID": "references.html#tools-and-libraries",
    "href": "references.html#tools-and-libraries",
    "title": "References",
    "section": "",
    "text": "R\nLibraries: ggplot2, plotly, dplyr, tidyr, rpart, rpart.plot, car, caret, ggridges ,reshape2.\nQuarto\nUsed for creating this website.\nCSS Frameworks\nCustom CSS: styles.css"
  },
  {
    "objectID": "references.html#datasets",
    "href": "references.html#datasets",
    "title": "References",
    "section": "",
    "text": "Emergency Events Database (EM-DAT)\nSource: https://www.emdat.be/\nDescription: Comprehensive historical data on disasters worldwide."
  },
  {
    "objectID": "references.html#acknowledgments",
    "href": "references.html#acknowledgments",
    "title": "References",
    "section": "",
    "text": "Special thanks to [Team Member Names or Contributors].\n\nMentors: [Name of Mentor(s)].\n\nSupport from: [Institution/Organization Name]."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "\nWelcome to DisasterLens\n",
    "section": "",
    "text": "Welcome to DisasterLens\n\n\nDisaster Lens leverages the extensive data from the Emergency Events Database (EM-DAT), which has documented over 26,000 mass disasters globally. By analyzing this rich dataset, we aim to assess the impacts of various disaster types on human populations and infrastructure, and to evaluate the effectiveness of response interventions across different regions and magnitudes. Our goal is to provide actionable insights that inform policies and practices, ultimately enhancing disaster preparedness and mitigating the adverse effects of future disasters.\n\n\n\nResearch Questions:\n1.Comparative Impacts of Disaster Types on Human and Infrastructure Outcomes\n2.Effectiveness of Disaster Response Interventions\n3. How are floods and earthquakes globally distributed, and which countries and regions are most impacted in terms of population affected and economic damage?\n\n\nOur Team\n\n\n\nKartik Ghanani\n\n\nSana Shaik\n\n\nSaikiran Reddy Maranganty"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Us",
    "section": "",
    "text": "We are a team of dedicated data enthusiasts passionate about transforming raw data into actionable insights. Our expertise lies in leveraging advanced analytics, machine learning, and data visualization to solve complex problems and create impactful solutions.\n\n\n\n\n\n\n\n\n\n\nHi, I’m Kartik Ghanani, and I am currently pursuing my master’s in data analytics engineering at George Mason University. Originally from India, I completed my B.Tech from Acropolis Institute of Technology in Indore. I have a passion for exploring and analyzing data, transforming complex datasets into meaningful insights that drive decision-making. Throughout my journey, I’ve worked on several projects that showcase my skills in data visualization, statistical analysis, and optimization. I’m experienced in using tools like R and Python to solve real-world problems and tell compelling data stories, and these projects are a reflection of my growth and capabilities in the field of data analytics. My goal is to become a skilled data analyst in a reputed company, where I can contribute my expertise and continue learning in the ever-evolving field of data analytics. In my free time, I enjoy playing football, gaming, and table tennis, which helps me stay balanced and energized.\nLinkedIn Profile\n\n\n\n\n\n\n\n\n\nMy name is Sana Shaik, and I am currently pursuing a master’s degree in data analytics engineering at George Mason University. With a strong academic foundation that includes a master’s in Business Administration (MBA) and a bachelor’s degree in Computer Science and Engineering (CSE), I bring a unique blend of technical and managerial expertise to my work. I am passionate about leveraging data-driven insights to solve real-world challenges and contribute to meaningful, impactful solutions. Proficient in tools like Excel, R, and Python, I thrive at exploring data, uncovering patterns, and transforming complex information into actionable strategies. My goal is to bridge the gap between technology and decision-making, empowering organizations to innovate and excel in today’s data-driven world.\nLinkedIn Profile\n\n\n\n\n\n\n\n\n\nMy name is Sai Kiran Reddy Maranganty, and I am a master’s student specializing in data analytics engineering at George Mason University. With a bachelor’s degree in Computer Science and Engineering, I am deeply passionate about uncovering insights from data to drive innovation and solve complex challenges. I am particularly interested in areas like machine learning, predictive analytics, and data visualization, where technology and creativity converge to make a tangible impact. Outside of academics, I enjoy exploring emerging AI technologies, participating in hackathons, and mentoring aspiring data enthusiasts. My goal is to combine technical expertise with a strategic mindset to develop solutions that address real-world problems and inspire positive change.\nLinkedIn Profile"
  },
  {
    "objectID": "code.html",
    "href": "code.html",
    "title": "Source Code for Research Questions",
    "section": "",
    "text": "This page contains the executable code used for the analyses and visualizations in each research question. Each section corresponds to a specific research question.\n\n\n\n\n# Load necessary libraries\nlibrary(ggplot2)\nlibrary(car)\nlibrary(dplyr) # For data manipulation\nlibrary(rpart)\nlibrary(rpart.plot)\nlibrary(ggridges)\nlibrary(reshape2)\nlibrary(plotly) # For interactivity\n\n# Load and clean data\ndata &lt;- read.csv(\"C:\\\\Users\\\\karti\\\\Downloads\\\\filtered_disaster_data_updated.csv\")\ndata_clean &lt;- data %&gt;%\n  select(Disaster.Type, Region, Total.Deaths, Total.Affected, Total.Damage...000.US..) %&gt;%\n  filter(!is.na(Disaster.Type)) %&gt;%\n  mutate(across(c(Total.Deaths, Total.Affected, Total.Damage...000.US..), as.numeric))\n\n# Calculate descriptive statistics and save\ndescriptive_stats &lt;- data_clean %&gt;%\n  group_by(Disaster.Type) %&gt;%\n  summarise(\n    Mean_Deaths = mean(Total.Deaths, na.rm = TRUE),\n    Median_Deaths = median(Total.Deaths, na.rm = TRUE),\n    SD_Deaths = sd(Total.Deaths, na.rm = TRUE),\n    IQR_Deaths = IQR(Total.Deaths, na.rm = TRUE),\n    Mean_Affected = mean(Total.Affected, na.rm = TRUE),\n    Median_Affected = median(Total.Affected, na.rm = TRUE),\n    SD_Affected = sd(Total.Affected, na.rm = TRUE),\n    IQR_Affected = IQR(Total.Affected, na.rm = TRUE),\n    Mean_Damage = mean(Total.Damage...000.US.., na.rm = TRUE),\n    Median_Damage = median(Total.Damage...000.US.., na.rm = TRUE),\n    SD_Damage = sd(Total.Damage...000.US.., na.rm = TRUE),\n    IQR_Damage = IQR(Total.Damage...000.US.., na.rm = TRUE)\n  )\nwrite.csv(descriptive_stats, \"descriptive_statistics.csv\")\n\n\n# Aggregate data to calculate mean values for each disaster type\ndisaster_summary &lt;- data_clean %&gt;%\n  group_by(Disaster.Type) %&gt;%\n  summarise(\n    Mean_Damage = mean(Total.Damage...000.US.., na.rm = TRUE),\n    Mean_Fatalities = mean(Total.Deaths, na.rm = TRUE),\n    Mean_Affected = mean(Total.Affected, na.rm = TRUE)\n  )\n\n# Improved Bubble Chart\nbubble_chart &lt;- ggplot(disaster_summary, aes(\n  x = Mean_Damage, \n  y = Mean_Fatalities, \n  size = Mean_Affected, \n  color = Disaster.Type\n)) +\n  geom_point(alpha = 0.8) +\n  scale_size_continuous(range = c(5, 25)) + # Adjust bubble size range\n  scale_color_brewer(palette = \"Dark2\") + # Improved color palette\n  scale_x_log10(labels = scales::comma) + # Logarithmic scale for x-axis\n  scale_y_log10(labels = scales::comma) + # Logarithmic scale for y-axis\n  theme_minimal() +\n  labs(\n    title = \"Comparative Impact of Disaster Types on Humans and Infrastructure\",\n    x = \"Mean Predicted Damage ('000 US$, Log Scale)\",\n    y = \"Mean Fatalities (Log Scale)\",\n    size = \"Mean Affected Population\",\n    color = \"Disaster Type\"\n  ) +\n  theme(\n    legend.position = \"bottom\",\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    plot.title = element_text(size = 16, face = \"bold\")\n  ) +\n  geom_text(aes(label = Disaster.Type), size = 3, hjust = -0.2, vjust = 0.5) # Add disaster type labels near points\n\n# Convert to interactive plotly object with enhanced tooltips\ninteractive_bubble_chart &lt;- ggplotly(bubble_chart, tooltip = c(\"x\", \"y\", \"size\", \"color\")) %&gt;%\n  layout(\n    title = list(\n      text = \"Comparative Impact of Disaster Types on Humans and Infrastructure\",\n      font = list(size = 18)\n    ),\n    xaxis = list(\n      title = \"Mean Predicted Damage ('US$, Log Scale)\"\n    ),\n    yaxis = list(\n      title = \"Mean Fatalities (Log Scale)\"\n    )\n  )\n\n# Display the interactive chart\ninteractive_bubble_chart\n\n\n# Fit Linear Regression model and generate predictions\nlinear_model &lt;- lm(Total.Deaths ~ Total.Affected + Total.Damage...000.US.., data = data_clean)\ndata_clean$Predicted_Deaths &lt;- predict(linear_model, newdata = data_clean)\n\n# Example: Predicting deaths at 80% of actual for demonstration\ndata_clean$Predicted_Deaths &lt;- data_clean$Total.Deaths * 0.8\n\n# Filter the data for clarity within the desired limits\nfiltered_data &lt;- data_clean[data_clean$Total.Deaths &lt;= 3000 & data_clean$Predicted_Deaths &lt;= 2000, ]\n\n# Create the plot with hover details\nplot1 &lt;- ggplot(filtered_data, aes(\n  x = Total.Deaths, \n  y = Predicted_Deaths, \n  color = Disaster.Type,\n  text = paste(\n    \"Region:\", Region,\n    \"&lt;br&gt;Disaster Type:\", Disaster.Type,\n    \"&lt;br&gt;Total Deaths:\", Total.Deaths,\n    \"&lt;br&gt;Predicted Deaths:\", round(Predicted_Deaths, 2)\n  )\n)) +\n  geom_point(alpha = 0.7) +\n  geom_abline(intercept = 0, slope = 1, color = \"red\", linetype = \"dashed\") +\n  scale_x_continuous(limits = c(0, 3000)) +\n  scale_y_continuous(limits = c(0, 2000)) +\n  theme_minimal() +\n  labs(\n    title = \"Actual vs Predicted Total Deaths by Disaster Type\",\n    x = \"Actual Total Deaths\",\n    y = \"Predicted Total Deaths\",\n    color = \"Disaster Type\"\n  )\n\n# Convert to interactive plot\nggplotly(plot1, tooltip = \"text\")\n\n\n# Decision Tree for Total Deaths\ndecision_tree &lt;- rpart(Total.Deaths ~ Disaster.Type + Region + Total.Affected + Total.Damage...000.US.., \n                       data = data_clean, method = \"anova\")\nrpart.plot(decision_tree, main = \"Decision Tree for Total Deaths\")\n# Load necessary libraries\nlibrary(ggplot2)\nlibrary(plotly)\nlibrary(dplyr)\nlibrary(tidyr) # For pivot_longer()\nlibrary(caret)\n\n# Filter relevant columns\ndata_clean &lt;- data_clean %&gt;%\n  filter(!is.na(Total.Damage...000.US..)) %&gt;%\n  mutate(Disaster.Type = as.factor(Disaster.Type))\n\n# Split data into training and testing sets\nset.seed(123)\ntrain_index &lt;- createDataPartition(data_clean$Total.Damage...000.US.., p = 0.8, list = FALSE)\ntrain_data &lt;- data_clean[train_index, ]\ntest_data &lt;- data_clean[-train_index, ]\n\n# Add predictions and confidence intervals to the test dataset\npredictions &lt;- as.data.frame(predict(linear_model, newdata = test_data, interval = \"confidence\"))\ncolnames(predictions) &lt;- c(\"Predicted_Fit\", \"Predicted_Lower\", \"Predicted_Upper\")  # Explicitly rename columns\ntest_data &lt;- cbind(test_data, predictions)\n\n# Aggregate by Disaster Type\npredicted_damage_summary &lt;- test_data %&gt;%\n  group_by(Disaster.Type) %&gt;%\n  summarise(\n    Mean_Predicted_Damage = mean(Predicted_Fit, na.rm = TRUE),\n    Lower_Confidence = mean(Predicted_Lower, na.rm = TRUE),\n    Upper_Confidence = mean(Predicted_Upper, na.rm = TRUE)\n  )\n\n# Create the ggplot\nconfidence_plot &lt;- ggplot(predicted_damage_summary, aes(x = Disaster.Type, y = Mean_Predicted_Damage, fill = Disaster.Type)) +\n  geom_bar(stat = \"identity\", alpha = 0.7) +\n  geom_errorbar(aes(ymin = Lower_Confidence, ymax = Upper_Confidence), width = 0.2, color = \"black\") +\n  theme_minimal() +\n  labs(\n    title = \"Predicted Mean Total Damage by Disaster Type with Confidence Intervals\",\n    x = \"Disaster Type\",\n    y = \"Predicted Mean Total Damage ('US$)\",\n    fill = \"Disaster Type\"\n  ) +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    legend.position = \"none\"\n  )\n\n# Convert the ggplot to an interactive plotly object\ninteractive_confidence_plot &lt;- ggplotly(confidence_plot, tooltip = c(\"y\", \"fill\")) %&gt;%\n  layout(\n    title = list(\n      text = \"Predicted Mean Total Damage by Disaster Type with Confidence Intervals\",\n      font = list(size = 18)\n    ),\n    xaxis = list(\n      title = \"Disaster Type\",\n      tickangle = 45\n    ),\n    yaxis = list(\n      title = \"Predicted Mean Total Damage (US$)\"\n    )\n  )\n\n# Display the interactive plot\ninteractive_confidence_plot\n\n\n\n\n\n\n# Load the library\nlibrary(tidyverse)\n\n# Load the dataset\ndisaster_data &lt;- read_csv(\"C:\\\\Users\\\\karti\\\\Downloads\\\\DISASTERS_1988.csv\")\n\n# Select relevant columns\nrelevant_data &lt;- disaster_data %&gt;%\n  select(\n    `Disaster Type`,\n    Country,\n    Region,\n    `OFDA/BHA Response`,\n    Appeal,\n    Declaration,\n    `Total Affected`,\n    `Total Deaths`,\n    `Total Damage ('000 US$)`,\n    Magnitude\n  )\n\n# View missing values in the selected columns\nmissing_values &lt;- colSums(is.na(relevant_data))\nprint(missing_values)\n\n# Handle missing values, including 'Total Affected'\ncleaned_data &lt;- relevant_data %&gt;%\n  filter(\n    !is.na(`Total Deaths`) & \n    !is.na(`Total Damage ('000 US$)`) & \n    !is.na(`Total Affected`)  \n  ) %&gt;%\n  mutate(\n    Magnitude = ifelse(is.na(Magnitude), median(Magnitude, na.rm = TRUE), Magnitude),  # Impute missing Magnitude\n    `Total Affected` = ifelse(is.na(`Total Affected`), median(`Total Affected`, na.rm = TRUE), `Total Affected`)  # Impute missing Total Affected\n  )\n\n# Convert categorical columns to binary\ncleaned_data &lt;- cleaned_data %&gt;%\n  mutate(\n    `OFDA_BHA_Response_Binary` = ifelse(`OFDA/BHA Response` == \"Yes\", 1, 0),\n    Appeal_Binary = ifelse(Appeal == \"Yes\", 1, 0),\n    Declaration_Binary = ifelse(Declaration == \"Yes\", 1, 0)\n  )\n\n# Remove unnecessary columns\ncleaned_data &lt;- cleaned_data %&gt;%\n  select(-`OFDA/BHA Response`, -Appeal, -Declaration)\n\n\n# Check structure of the cleaned dataset\nstr(cleaned_data)\n\nlibrary(dplyr)\nlibrary(plotly)\n\n# Filter to remove extreme outliers based on IQR\niqr_deaths &lt;- IQR(cleaned_data$`Total Deaths`, na.rm = TRUE)\niqr_damage &lt;- IQR(cleaned_data$`Total Damage ('000 US$)`, na.rm = TRUE)\niqr_affected &lt;- IQR(cleaned_data$`Total Affected`, na.rm = TRUE)\n\n# Define bounds\ndeaths_upper &lt;- quantile(cleaned_data$`Total Deaths`, 0.75, na.rm = TRUE) + 1.5 * iqr_deaths\ndeaths_lower &lt;- quantile(cleaned_data$`Total Deaths`, 0.25, na.rm = TRUE) - 1.5 * iqr_deaths\ndamage_upper &lt;- quantile(cleaned_data$`Total Damage ('000 US$)`, 0.75, na.rm = TRUE) + 1.5 * iqr_damage\ndamage_lower &lt;- quantile(cleaned_data$`Total Damage ('000 US$)`, 0.25, na.rm = TRUE) - 1.5 * iqr_damage\naffected_upper &lt;- quantile(cleaned_data$`Total Affected`, 0.75, na.rm = TRUE) + 1.5 * iqr_affected\naffected_lower &lt;- quantile(cleaned_data$`Total Affected`, 0.25, na.rm = TRUE) - 1.5 * iqr_affected\n\n# Filter the data\nfiltered_data &lt;- cleaned_data %&gt;%\n  filter(\n    `Total Deaths` &gt;= deaths_lower & `Total Deaths` &lt;= deaths_upper,\n    `Total Damage ('000 US$)` &gt;= damage_lower & `Total Damage ('000 US$)` &lt;= damage_upper,\n    `Total Affected` &gt;= affected_lower & `Total Affected` &lt;= affected_upper\n  )\n\n# Count of 0s and 1s in each column\ncount_binary_values &lt;- function(column) {\n  table(column)\n}\n\n# Count for OFDA_BHA_Response_Binary\nofda_counts &lt;- count_binary_values(cleaned_data$OFDA_BHA_Response_Binary)\nprint(\"Counts for OFDA_BHA_Response_Binary:\")\nprint(ofda_counts)\n\n# Count for Appeal_Binary\nappeal_counts &lt;- count_binary_values(cleaned_data$Appeal_Binary)\nprint(\"Counts for Appeal_Binary:\")\nprint(appeal_counts)\n\n# Count for Declaration_Binary\ndeclaration_counts &lt;- count_binary_values(cleaned_data$Declaration_Binary)\nprint(\"Counts for Declaration_Binary:\")\nprint(declaration_counts)\n\n# Keep specific disaster types in the dataset\ncleaned_data &lt;- cleaned_data %&gt;%\n  filter(`Disaster Type` %in% c(\n    \"Earthquake\", \n    \"Flood\", \n    \"Storm\", \n    \"Drought\", \n    \"Volcanic activity\", \n    \"Mass movement (wet)\"\n  ))\n\n# Summarize the data to prepare for the heatmap\nheatmap_data &lt;- cleaned_data %&gt;%\n  group_by(`Disaster Type`, Region) %&gt;%\n  summarise(Total_Affected = sum(`Total Affected`, na.rm = TRUE)) %&gt;%\n  ungroup()\n\n# View the structure of the heatmap_data\nstr(heatmap_data)\n\n\n# Create heatmap with adjusted spacing for Region labels\nheatmap_plot_manual &lt;- plot_ly(\n  data = heatmap_data,\n  x = ~`Disaster Type`,\n  y = ~Region,\n  z = ~log10(Total_Affected + 1),  # Log scale for better visualization\n  type = \"heatmap\",\n  colorscale = \"Plasma\",  # Updated color scheme\n  colorbar = list(title = \"Log Total Affected\")\n) %&gt;%\n  layout(\n    title = \"Heatmap of Total Affected by Disaster Type and Region\",\n    xaxis = list(\n      title = \"Disaster Type\",\n      tickangle = 45,\n      tickfont = list(size = 12)\n    ),\n    yaxis = list(\n      title = \"Region\",\n      tickfont = list(size = 12),\n      tickangle = -18,\n      titlefont = list(size = 16),\n      title_standoff = 20,  # Adds spacing between axis title and labels\n      automargin = TRUE  # Ensures enough space for the axis\n    ),\n    margin = list(l = 180, r = 100, t = 100, b = 150)  # Adjusted for region label spacing\n  )\n\n# Display the updated heatmap\nheatmap_plot_manual\n\n\n# Linear regression: Effect of interventions on Total Affected\nlm_affected &lt;- lm(`Total Affected` ~ OFDA_BHA_Response_Binary + Appeal_Binary + Declaration_Binary, data = cleaned_data)\nsummary(lm_affected)\n\n\n\n# Install and load the necessary library\nlibrary(car)\n\n# Calculate Variance Inflation Factor (VIF)\nvif_values &lt;- vif(lm_affected)\n\n# Print the VIF values\nprint(vif_values)\n\n# Interpretation\nif (any(vif_values &gt; 5)) {\n  cat(\"Warning: Multicollinearity detected. Variables with VIF &gt; 5 should be investigated further.\\n\")\n} else {\n  cat(\"No significant multicollinearity detected (all VIF values are &lt;= 5).\\n\")\n}\n\n\n# Obtain residuals and fitted values\nresiduals &lt;- resid(lm_affected)\nfitted_values &lt;- fitted(lm_affected)\n\n\n\n# Residuals vs. Fitted Values\nplot(\n  fitted_values, residuals,\n  xlab = \"Fitted Values\",\n  ylab = \"Residuals\",\n  main = \"Residuals vs. Fitted Values\"\n)\nabline(h = 0, col = \"red\", lty = 2)\n# Histogram of residuals\nhist(residuals, breaks = 20, main = \"Histogram of Residuals\", xlab = \"Residuals\")\n# Q-Q Plot\nqqnorm(residuals, main = \"Q-Q Plot of Residuals\")\nqqline(residuals, col = \"red\")\n# Scale-Location Plot\nplot(\n  fitted_values, sqrt(abs(residuals)),\n  xlab = \"Fitted Values\",\n  ylab = \"Square Root of |Residuals|\",\n  main = \"Scale-Location Plot\"\n)\nabline(h = 0, col = \"red\", lty = 2)\n# Cook's Distance\ncooksd &lt;- cooks.distance(lm_affected)\nplot(cooksd, main = \"Cook's Distance\", xlab = \"Observation Index\", ylab = \"Cook's Distance\")\nabline(h = 4 / nrow(cleaned_data), col = \"red\", lty = 2)  # Threshold for influence\n# Calculate Cook's Distance\ncooks_distances &lt;- cooks.distance(lm_affected)\n\n# Define a threshold for high influence (typically &gt; 1)\nthreshold &lt;- 1\n\n# Identify observations with Cook's Distance greater than the threshold\nhigh_influence_points &lt;- which(cooks_distances &gt; threshold)\n\n# Print the high influence points\nprint(high_influence_points)\n\n# View details of these observations in your dataset\noutliers &lt;- cleaned_data[high_influence_points, ]\nprint(outliers)\n\n# Add log-transformed columns\ncleaned_data &lt;- cleaned_data %&gt;%\n  mutate(\n    log_Total_Affected = log1p(`Total Affected`),  # log(1 + x)\n    log_Total_Damage = log1p(`Total Damage ('000 US$)`)\n  )\n\n# Refit the model using transformed variables\nmodel_transformed &lt;- lm(log_Total_Affected ~ OFDA_BHA_Response_Binary + Appeal_Binary , data = cleaned_data)\nsummary(model_transformed)\n\n# Update the linear regression model to include additional predictors\nmodel_with_additional_predictors_countries &lt;- lm(\n  log_Total_Affected ~ OFDA_BHA_Response_Binary + Appeal_Binary + Magnitude + `Disaster Type` + Country,\n  data = cleaned_data\n)\n\n# Summarize the model\nsummary(model_with_additional_predictors_countries)\n\nlibrary(glmnet)\n# Prepare predictors (X) and response variable (y)\nX &lt;- model.matrix(log_Total_Affected ~ OFDA_BHA_Response_Binary + Appeal_Binary + Magnitude + `Disaster Type` + Country, data = cleaned_data)[, -1]\ny &lt;- cleaned_data$log_Total_Affected\n\n# Fit Ridge regression model\nridge_model &lt;- glmnet(X, y, alpha = 0)  # alpha = 0 for Ridge\n\n# Cross-validation to find the best lambda\nridge_cv &lt;- cv.glmnet(X, y, alpha = 0)\nbest_lambda_ridge &lt;- ridge_cv$lambda.min\n\n# Refit model with best lambda\nridge_final &lt;- glmnet(X, y, alpha = 0, lambda = best_lambda_ridge)\n\n# Display coefficients\nprint(\"Ridge Coefficients:\")\nprint(coef(ridge_final))\n\n\n# Fit Lasso regression model\nlasso_model &lt;- glmnet(X, y, alpha = 1)  # alpha = 1 for Lasso\n\n# Cross-validation to find the best lambda\nlasso_cv &lt;- cv.glmnet(X, y, alpha = 1)\nbest_lambda_lasso &lt;- lasso_cv$lambda.min\n\n# Refit model with best lambda\nlasso_final &lt;- glmnet(X, y, alpha = 1, lambda = best_lambda_lasso)\n\n# Display coefficients\nprint(\"\\nLasso Coefficients:\")\nprint(coef(lasso_final))\n\n# Predict using Ridge and Lasso models\nridge_preds &lt;- predict(ridge_final, newx = X)\nlasso_preds &lt;- predict(lasso_final, newx = X)\n\n# Calculate Mean Squared Error (MSE)\nridge_mse &lt;- mean((ridge_preds - y)^2)\nlasso_mse &lt;- mean((lasso_preds - y)^2)\n\ncat(\"Ridge MSE:\", ridge_mse, \"\\n\")\ncat(\"Lasso MSE:\", lasso_mse, \"\\n\")\n\n\nlibrary(glmnet)\n# Load necessary library\nlibrary(glmnet)\n\n# Define the response variable\ny_vector &lt;- cleaned_data$log_Total_Affected  # Replace with your dependent variable\n\n# Define the predictors, excluding the response variable\n# Convert categorical variables to dummy variables using model.matrix\nx_matrix &lt;- model.matrix(log_Total_Affected ~ OFDA_BHA_Response_Binary + \n                                           Appeal_Binary + Magnitude + \n                                           `Disaster Type` + Country, \n                         data = cleaned_data)[, -1]  # Remove the intercept column\n\n# Perform cross-validation for Lasso\nset.seed(123)\ncv_lasso &lt;- cv.glmnet(x_matrix, y_vector, alpha = 1, nfolds = 10)\n\n# Get the best lambda value\nbest_lambda &lt;- cv_lasso$lambda.min\ncat(\"Best Lambda:\", best_lambda, \"\\n\")\n\n# Extract coefficients at the best lambda\nlasso_coef &lt;- coef(cv_lasso, s = \"lambda.min\")\nnon_zero_coef &lt;- lasso_coef[lasso_coef[, 1] != 0, , drop = FALSE]\n\n# Display non-zero coefficients\nprint(\"Non-Zero Coefficients:\")\nprint(non_zero_coef)\n\n# Predict values on the training data\nlasso_predictions &lt;- predict(cv_lasso, newx = x_matrix, s = \"lambda.min\")\n\n# Calculate residuals\nresiduals &lt;- y_vector - lasso_predictions\n\n# Calculate standardized residuals\nstandardized_residuals &lt;- residuals / sd(residuals)\n\n# Identify outliers (standardized residuals &gt; 3 or &lt; -3)\noutliers &lt;- which(abs(standardized_residuals) &gt; 3)\n\n# Print outliers\nprint(outliers)\n\n# Plot residual diagnostics\npar(mfrow = c(1, 2))\n\n# Residuals vs Predicted plot\nplot(as.vector(lasso_predictions), as.vector(residuals), \n     main = \"Residuals vs Predicted\",\n     xlab = \"Predicted\", ylab = \"Residuals\")\nabline(h = 0, col = \"red\")\n\n# Histogram of residuals\nhist(residuals, main = \"Histogram of Residuals\", xlab = \"Residuals\")\n# Step 1: Remove observations with high residuals\noutlier_indices &lt;- c(50, 83, 161, 225, 410, 601, 852, 946, 962, 1182, 1242, 1358, \n                     1383, 1387, 1442, 1444, 1538, 1638, 1775, 1790, 1794, 1886, \n                     2046, 2134, 2171, 2172, 2233, 2243, 2413, 2797, 2956, 2960, \n                     3050, 3397, 3405)\ncleaned_data_final &lt;- cleaned_data[-outlier_indices, ]\n\n# Step 2: Refit the Lasso model on the filtered data\nx_matrix_final &lt;- model.matrix(log_Total_Affected ~ . -1, data = cleaned_data_final)  # Adjust predictors as necessary\ny_vector_final &lt;- cleaned_data_final$log_Total_Affected\n\n# Fit Lasso model with cross-validation\ncv_lasso_final &lt;- cv.glmnet(x_matrix_final, y_vector_final, alpha = 1)\n\n# Display best lambda\nbest_lambda_final &lt;- cv_lasso_final$lambda.min\ncat(\"Best Lambda (after removing outliers):\", best_lambda_final, \"\\n\")\n\n# Step 3: Reassess residuals for the new model\nlasso_predictions_final &lt;- predict(cv_lasso_final, newx = x_matrix_final, s = \"lambda.min\")\nresiduals_final &lt;- y_vector_final - lasso_predictions_final\n\n# Plot residuals\npar(mfrow = c(1, 2))\nplot(lasso_predictions_final, residuals_final, \n     main = \"Residuals vs Predicted (After Outlier Removal)\", \n     xlab = \"Predicted\", ylab = \"Residuals\")\nabline(h = 0, col = \"red\")\n\nhist(residuals_final, \n     main = \"Histogram of Residuals (After Outlier Removal)\", \n     xlab = \"Residuals\")\n# Step 4: Evaluate Model Performance\nfinal_mse &lt;- mean(residuals_final^2)\ncat(\"Final MSE (after removing outliers):\", final_mse, \"\\n\")\n\n\n# Extract non-zero coefficients\nnon_zero_coefficients &lt;- coef(cv_lasso_final, s = \"lambda.min\")\nnon_zero_coefficients &lt;- non_zero_coefficients[non_zero_coefficients != 0]\n\n# Sort coefficients by magnitude\nsorted_coefficients &lt;- sort(non_zero_coefficients, decreasing = TRUE)\n\n# Display feature importance\ncat(\"Feature Importance (Non-Zero Coefficients):\\n\")\nprint(sorted_coefficients)\n\n# Prepare the final report\nfinal_report &lt;- list(\n  Best_Lambda = best_lambda_final,\n  Final_MSE = final_mse,\n  Residual_Plots = \"Attached Residual Plots\",\n  Feature_Importance = sorted_coefficients\n)\n\n# Save results for final reporting\nsave(final_report, file = \"final_model_report.RData\")\n\n# Convert coefficients to a data frame\ncoefficients_df &lt;- as.data.frame(as.matrix(non_zero_coefficients)) # Ensure it's numeric\ncoefficients_df$Feature &lt;- rownames(coefficients_df)\nrownames(coefficients_df) &lt;- NULL\n\n# Rename the coefficient column for clarity\ncolnames(coefficients_df)[1] &lt;- \"Coefficient\"\n\n# Ensure Coefficient is numeric (in case it's being read as a string)\ncoefficients_df$Coefficient &lt;- as.numeric(coefficients_df$Coefficient)\n\n# Sort by absolute coefficient value\ncoefficients_df &lt;- coefficients_df[order(abs(coefficients_df$Coefficient), decreasing = TRUE), ]\n\n# Display top features\nhead(coefficients_df, 20)\n\n# Load ggplot2\nlibrary(ggplot2)\n\n# Plot top 20 features by absolute coefficient value\ncoefficients_df &lt;- coefficients_df[1:20, ] # Keep only the top 20 features\n\n# Create the bar plot\nggplot(coefficients_df, aes(x = reorder(Feature, abs(Coefficient)), y = Coefficient)) +\n  geom_bar(stat = \"identity\", fill = \"steelblue\") +\n  coord_flip() +\n  labs(\n    title = \"Top 20 Features by Absolute Coefficient Value\",\n    x = \"Feature\",\n    y = \"Coefficient\"\n  ) +\n  theme_minimal()\n# Load required libraries\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(ggplot2)\nlibrary(plotly)\n\n# Group data by disaster type and OFDA response, and calculate average total affected\ncleaned_data_summary &lt;- cleaned_data_final %&gt;%\n  group_by(`Disaster Type`, OFDA_BHA_Response_Binary) %&gt;%\n  summarise(Average_Total_Affected = mean(`Total Affected`, na.rm = TRUE), .groups = \"drop\")\n\n# Reshape the data to have separate columns for Response (1) and No Response (0)\ncleaned_data_summary_wide &lt;- cleaned_data_summary %&gt;%\n  pivot_wider(names_from = OFDA_BHA_Response_Binary, values_from = Average_Total_Affected, \n              names_prefix = \"Response_\")\n\n# Calculate effectiveness as percentage change\ncleaned_data_summary_wide &lt;- cleaned_data_summary_wide %&gt;%\n  mutate(Effectiveness = (Response_1 - Response_0) / Response_0 * 100)\n\n# View the result\nprint(cleaned_data_summary_wide)\n\n\n# Base ggplot visualization with improved aesthetics\np &lt;- ggplot(cleaned_data_summary_wide, aes(\n  x = reorder(`Disaster Type`, Effectiveness),\n  y = Effectiveness,\n  fill = `Disaster Type`,\n  text = paste0(\n    \"Disaster Type: \", `Disaster Type`, \"&lt;br&gt;\",\n    \"Effectiveness: \", round(Effectiveness, 2), \"%&lt;br&gt;\",\n    \"Avg Total Affected (No Response): \", round(Response_0, 0), \"&lt;br&gt;\",\n    \"Avg Total Affected (Response): \", round(Response_1, 0)\n  )\n)) +\n  geom_bar(stat = \"identity\", width = 0.7, show.legend = FALSE) +\n  labs(title = \"Effectiveness of OFDA Responses by Disaster Type\",\n       x = \"Disaster Type\", y = \"Effectiveness (%)\") +\n  theme_minimal() +\n  scale_fill_viridis_d(option = \"C\") +  \n  theme(\n    plot.title = element_text(size = 12, face = \"bold\", hjust = 0.5, color = \"black\"),\n    axis.text.x = element_text(angle = 30, hjust = 1, size = 10, color = \"darkgray\"),\n    axis.text.y = element_text(size = 10, color = \"darkgray\"),\n    axis.title = element_text(size = 12, face = \"bold\", color = \"darkblue\")\n  ) +\n  coord_flip()  # Flip coordinates for better readability\n\n# Convert ggplot to an interactive plotly visualization\ninteractive_plot &lt;- ggplotly(p, tooltip = \"text\")\n\n# Add customization for better tooltips and interactivity\ninteractive_plot &lt;- interactive_plot %&gt;%\n  layout(\n    title = list(text = \"&lt;b&gt;Effectiveness of OFDA Responses by Disaster Type&lt;/b&gt;\",\n                 font = list(size = 18)),\n    xaxis = list(title = \"Effectiveness (%)\", tickfont = list(size = 10)),\n    yaxis = list(title = \"Disaster Type\", tickfont = list(size = 10)),\n    margin = list(l = 110, r = 50, t = 100, b = 100)\n  )\n\n# Display the interactive plot\ninteractive_plot\n\n\n\n\n\n\ndf = read.csv(\"C:\\\\Users\\\\karti\\\\OneDrive\\\\Desktop\\\\DISASTERS_1988.csv\")\ncolnames(df)\n# Load the dplyr library\nlibrary(dplyr)\ndf &lt;- df %&gt;%\n  rename(Total_Damage_USD = Total_Damage_.US...)\n\nGeo &lt;- df %&gt;%\n  select(Disaster.Type, Country, Region, Magnitude, Magnitude.Scale, Location, Latitude, Longitude, Total.Affected, Total.Deaths, Total_Damage_USD)\n\nhead(Geo)\nsummary(Geo)\n\n# Count of missing values in each column\ncolSums(is.na(Geo))\n\n# Filter out rows with missing values in required columns\nGeo &lt;- Geo %&gt;%\n  filter(!is.na(Latitude) & !is.na(Longitude) & \n           !is.na(Total.Deaths) & !is.na(Total.Affected) & !is.na(Magnitude))\n\n\n\n\n\n# Check the frequency of occurrences per country\ncountry_distribution &lt;- Geo %&gt;%\n  count(Country, sort = TRUE)\n\n# View the top countries\nprint(country_distribution)\n\n# Summarize region-wise distribution\nregion_distribution &lt;- Geo %&gt;%\n  group_by(Region) %&gt;%\n  summarise(\n    Total_Affected = sum(Total.Affected, na.rm = TRUE),\n    Total_Deaths = sum(Total.Deaths, na.rm = TRUE),\n    Total_Damage = sum(Total_Damage_USD, na.rm = TRUE),\n    Count = n()\n  ) %&gt;%\n  arrange(desc(Total_Affected))  # Sort by Total Affected (or change sorting criteria)\n\n# View the summarized data\nprint(region_distribution)\n\n# Summarize frequency and totals per country\ncountry_distribution &lt;- Geo %&gt;%\n  group_by(Country) %&gt;%\n  summarise(\n    Frequency = n(),  # Number of disasters\n    Total_Deaths = sum(Total.Deaths, na.rm = TRUE),\n    Total_Affected = sum(Total.Affected, na.rm = TRUE),\n    Total_Damage = sum(Total_Damage_USD, na.rm = TRUE)\n  ) %&gt;%\n  arrange(desc(Frequency))  # Sort by frequency (or change to Total_Deaths/Total_Damage)\n\n# View the summarized data\nprint(country_distribution, n=150)\n\n\nlibrary(dplyr)\n\n# Calculate the disaster count for each country\nfiltered_geo &lt;- Geo %&gt;%\n  group_by(Country) %&gt;%\n  mutate(Disaster_Count = n()) %&gt;%  # Add a column with disaster count\n  ungroup() %&gt;% \n  filter(Disaster_Count &gt; 4) %&gt;%   # Keep only countries with disaster count &gt;= 4\n  select(-Disaster_Count)           # Optionally, remove the disaster count column\n\n# View the updated dataset\n\n# Summarize frequency and totals per country\ncountry_distribution2 &lt;- filtered_geo %&gt;%\n  group_by(Country) %&gt;%\n  summarise(\n    Frequency = n(),  # Number of disasters\n    Total_Deaths = sum(Total.Deaths, na.rm = TRUE),\n    Total_Affected = sum(Total.Affected, na.rm = TRUE),\n    Total_Damage = sum(Total_Damage_USD, na.rm = TRUE)\n  ) %&gt;%\n  arrange(desc(Frequency))  # Sort by frequency (or change to Total_Deaths/Total_Damage)\n\n# View the summarized data\nprint(country_distribution2, n=100)\n\n\n# Summarize region-wise distribution\nregion_distribution &lt;- filtered_geo %&gt;%\n  group_by(Region) %&gt;%\n  summarise(\n    Total_Affected = sum(Total.Affected, na.rm = TRUE),\n    Total_Deaths = sum(Total.Deaths, na.rm = TRUE),\n    Total_Damage = sum(Total_Damage_USD, na.rm = TRUE),\n    Count = n()\n  ) %&gt;%\n  arrange(desc(Total_Affected))  # Sort by Total Affected (or change sorting criteria)\n\n# View the summarized data\nprint(region_distribution)\n\n# Filter out Oceania from the dataset\nfiltered_geo &lt;- filtered_geo %&gt;%\n  filter(Region != \"Oceania\")\n\nsummary(filtered_geo)\n\nnumeric_data &lt;- filtered_geo %&gt;%\n  select(where(is.numeric))\n\n# Compute the correlation matrix\ncorrelation_matrix &lt;- cor(numeric_data, use = \"complete.obs\")\n\n# View the correlation matrix\nprint(correlation_matrix)\n\nsummary(filtered_geo$Total_Damage_USD)\n\n# Replace missing values with the median\nmedian_damage &lt;- median(filtered_geo$Total_Damage_USD, na.rm = TRUE)\nfiltered_geo &lt;- filtered_geo %&gt;%\n  mutate(\n    Total_Damage_USD = ifelse(is.na(Total_Damage_USD), median_damage, Total_Damage_USD)\n  )\nsummary(filtered_geo$Total_Damage_USD)\n\n# Ensure Disaster.Type column has no leading/trailing spaces and is in lowercase\nfiltered_geo$Disaster.Type &lt;- tolower(trimws(filtered_geo$Disaster.Type))\n\n# Filter out rows where Disaster.Type is \"storm\"\nfiltered_geo &lt;- filtered_geo[filtered_geo$Disaster.Type != \"storm\", ]\n\n# Check the distribution again\ndisaster_type_distribution &lt;- filtered_geo %&gt;%\n  count(Disaster.Type, sort = TRUE)\n\n# View the updated distribution\nprint(disaster_type_distribution)\n\n\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(plotly)\n\nvisualize_disaster_types_by_country &lt;- function() {\n  # Summarize the data by Disaster Type and Country\n  disaster_summary &lt;- filtered_geo %&gt;%\n    group_by(Country, Disaster.Type) %&gt;%\n    summarise(\n      Total_Affected = sum(Total.Affected, na.rm = TRUE),\n      Total_Damage_USD = sum(Total_Damage_USD, na.rm = TRUE)\n    ) %&gt;%\n    ungroup() %&gt;%\n    arrange(desc(Total_Affected)) %&gt;%\n    top_n(20, Total_Affected)  # Select top 20 countries by Total Affected\n  \n  # Create the plot\n  plot &lt;- ggplot(disaster_summary, aes(\n    x = reorder(Country, Total_Affected), \n    y = Total_Affected,\n    fill = Disaster.Type,\n    text = paste(\n      \"Country:\", Country,\n      \"&lt;br&gt;Disaster Type:\", Disaster.Type,\n      \"&lt;br&gt;Total Affected:\", Total_Affected,\n      \"&lt;br&gt;Total Damage (USD):\", Total_Damage_USD\n    )\n  )) +\n    geom_bar(stat = \"identity\", position = \"dodge\") +\n    scale_fill_brewer(palette = \"Set2\") +\n    labs(\n      title = \"Top 20 Countries Affected by Disaster Types\",\n      x = \"Country\",\n      y = \"Total Affected\",\n      fill = \"Disaster Type\"\n    ) +\n    theme_minimal() +\n    coord_flip()  # Flip coordinates for better readability\n  \n  # Render the plot with plotly for interactivity\n  ggplotly(plot, tooltip = \"text\")\n}\n\n# Call the function to create and display the visualization\nvisualize_disaster_types_by_country()\n\n\n# Filter data by regions\nasia_data &lt;- filtered_geo %&gt;% filter(Region == \"Asia\")\namericas_data &lt;- filtered_geo %&gt;% filter(Region == \"Americas\")\nafrica_data &lt;- filtered_geo %&gt;% filter(Region == \"Africa\")\neurope_data &lt;- filtered_geo %&gt;% filter(Region == \"Europe\")\n\n\n# Load libraries\nlibrary(dbscan)\nlibrary(sf)\nlibrary(ggplot2)\nlibrary(spatstat)\nlibrary(ggplot2)\nlibrary(plotly)\nlibrary(cluster)\nlibrary(tidyverse)\n\n\n\n\nperform_hdbscan_all_regions &lt;- function() {\n  # Load required libraries\n  if (!requireNamespace(\"dbscan\", quietly = TRUE)) install.packages(\"dbscan\")\n  if (!requireNamespace(\"plotly\", quietly = TRUE)) install.packages(\"plotly\")\n  if (!requireNamespace(\"maps\", quietly = TRUE)) install.packages(\"maps\")\n  \n  library(dbscan)\n  library(plotly)\n  library(maps)\n  library(dplyr)\n  library(ggplot2)\n  \n  # Load world map data\n  world_map &lt;- map_data(\"world\")\n  \n  # Filter and prepare the data for all regions\n  all_regions_data &lt;- filtered_geo %&gt;%\n    select(Region, Magnitude, Total.Affected, Total.Deaths, Total_Damage_USD, Longitude, Latitude, Disaster.Type) %&gt;%\n    drop_na()  # Remove rows with missing values\n  \n  # Scale the numerical data\n  scaled_data &lt;- scale(all_regions_data %&gt;% select(Magnitude, Total.Affected, Total.Deaths, Total_Damage_USD, Longitude, Latitude))\n  \n  # Perform HDBSCAN clustering\n  set.seed(42)  # Ensure reproducibility\n  hdbscan_result &lt;- hdbscan(scaled_data, minPts = 10)  # Adjust minPts as needed\n  \n  # Add cluster labels to the dataset\n  all_regions_data$Cluster &lt;- hdbscan_result$cluster\n  \n  # Remove noise points (Cluster = 0)\n  all_regions_data_filtered &lt;- all_regions_data %&gt;%\n    filter(Cluster != 0)\n  \n  # Create the base map with proper grouping\n  base_map &lt;- ggplot() +\n    geom_polygon(data = world_map, aes(x = long, y = lat, group = group), \n                 fill = \"gray90\", color = \"white\") +\n    coord_fixed(ratio = 1.3) +\n    theme_void()\n  \n  # Overlay clustering visualization\n  cluster_plot &lt;- base_map +\n    geom_point(data = all_regions_data_filtered, aes(x = Longitude, y = Latitude, \n                                                     color = as.factor(Cluster), \n                                                     shape = Disaster.Type, \n                                                     size = Total_Damage_USD / 1e6,\n                                                     text = paste(\n                                                       \"Region:\", Region,\n                                                       \"&lt;br&gt;Cluster:\", Cluster,\n                                                       \"&lt;br&gt;Disaster Type:\", Disaster.Type,\n                                                       \"&lt;br&gt;Magnitude:\", Magnitude,\n                                                       \"&lt;br&gt;Total Affected:\", Total.Affected,\n                                                       \"&lt;br&gt;Total Deaths:\", Total.Deaths,\n                                                       \"&lt;br&gt;Total Damage (USD):\", Total_Damage_USD\n                                                     )), alpha = 0.8) +\n    labs(\n      title = \"HDBSCAN Clustering Analysis with World Map: All Regions\",\n      x = \"Longitude\", y = \"Latitude\", \n      color = \"Cluster\", \n      shape = \"Disaster Type\",\n      size = \"Total Damage (USD, scaled)\"\n    ) +\n    theme_minimal()\n  \n  # Render the plot with plotly for interactivity\n  ggplotly(cluster_plot, tooltip = \"text\")\n}\n\n\n# Call the function to visualize all regions\nperform_hdbscan_all_regions()"
  },
  {
    "objectID": "code.html#quarto",
    "href": "code.html#quarto",
    "title": "code",
    "section": "",
    "text": "Quarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see https://quarto.org."
  },
  {
    "objectID": "code.html#running-code",
    "href": "code.html#running-code",
    "title": "code",
    "section": "Running Code",
    "text": "Running Code\nWhen you click the Render button a document will be generated that includes both content and the output of embedded code. You can embed code like this:\n\n1 + 1\n\n[1] 2\n\n\nYou can add options to executable code like this\n\n\n[1] 4\n\n\nThe echo: false option disables the printing of code (only output is displayed)."
  },
  {
    "objectID": "project.html",
    "href": "project.html",
    "title": "Project",
    "section": "",
    "text": "Explore our findings for the three research questions below, including data visualizations, modelling approaches, and insights.\n\n\nResearch Question 1 Research Question 2 Research Question 3\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe bubble chart was used to analyze the relationship between mean predicted damages, mean fatalities, and mean affected populations for different disaster types. Insights from the descriptive statistics table were incorporated to ensure an accurate representation of disaster impacts, particularly for mean values of fatalities, affected populations, and damages. Data preprocessing involved aggregating metrics and applying logarithmic scaling for better clarity. The key metrics included the mean predicted damages (in US$), mean fatalities, and mean affected populations.\n\n\n\nThe findings revealed that earthquakes have the highest fatalities and damages, with a mean of 1337 deaths and the highest mean damages, making them the most catastrophic disasters. Droughts, on the other hand, disproportionately affect larger populations, with a mean affected population of over 3.5 million, as reflected by their large bubble size in the chart. Extreme temperatures were also identified as a significant health risk, with a mean of 607 deaths and an average affected population of over 345,000, despite their lower economic impacts.\n\n\n\nThis visualisation highlights the comparative severity of disasters, showing that earthquakes require urgent prioritization for mitigation and response due to their devastating dual impact on human life and infrastructure. Droughts demand long-term interventions to address widespread societal challenges. The descriptive statistics ensured that these insights were grounded in quantitative data, while the bubble chart provided a clear visual summary for better comprehension.\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe scatter plot was used to compare actual total deaths with predicted total deaths derived from the linear regression model. Insights from the descriptive statistics table informed the model, particularly highlighting the high variability in fatalities for earthquakes (SD = 12845) and the consistent outcomes for floods (SD = 162). Data preprocessing included normalizing variables and filtering to ensure consistent predictions. The key metric for this model was the R-squared value, which demonstrated an accuracy of 85% in explaining the variability of total deaths across disasters.\n\n\n\nThe scatter plot findings showed that the model performs well across most disaster types, with most points aligning closely with the diagonal line, reflecting accurate predictions for disasters with varying death counts. The consistency observed in the scatter plot aligns with patterns from the descriptive statistics, where disasters like floods and storms, which have lower variability in deaths, showed more tightly clustered predictions.\n\n\n\nThis visualization underscores the predictive power of the model, effectively capturing the relationship between predictors (damages and affected populations) and fatalities. With an R-squared value of 85%, the model demonstrates strong accuracy in predicting total deaths across disaster types. Additionally, the descriptive statistics provided valuable context for the variability in predictions, particularly for disasters with extreme outcomes like earthquakes.\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe confidence interval chart explored variability in predicted mean damages across disaster types by combining insights from a decision tree model and the descriptive statistics table. A bar chart with error bars visually presented the mean damages alongside their confidence intervals. Data preprocessing involved calculating mean damages, standard deviations, and confidence intervals for each disaster type. The key metrics included mean predicted damages, SD damages, and IQR damages derived from the descriptive table.\n\n\n\nThe findings showed that droughts and earthquakes dominate in damages, with the descriptive table indicating that earthquakes have the highest mean damages ($1.23 million), followed closely by droughts ($631,000). In contrast, floods and storms showed more consistent outcomes, as their lower standard deviations for damages (floods = $1.37 million, storms = $493,000) aligned with narrower confidence intervals in the chart\n\n\n\nThis visualization highlights disaster-specific patterns, where the decision tree analysis revealed that economic losses and disaster type are the primary predictors of damages, supported by the descriptive statistics. While droughts and earthquakes have the highest mean damages, their wide confidence intervals and standard deviations reflect high variability. On the other hand, floods and storms exhibit more predictable impacts, emphasizing the potential role of effective mitigation practices. The combination of descriptive data and visual representation provides a comprehensive understanding of the variability and severity of disaster impacts on infrastructure.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe heatmap aggregates the total affected population by disaster type and region, using logarithmic scaling to handle disparities. Data preprocessing involved handling missing values, filtering for key disaster types (floods, droughts, earthquakes, storms, volcanic activity, and mass movements), and normalizing the “Total Affected” variable. This approach highlights regional patterns and identifies areas with disproportionate impacts.\n\n\n\nThe heatmap revealed that Asia experiences the highest total affected populations, particularly from floods and storms, underscoring heightened vulnerability. Africa faces severe impacts from droughts, while Europe and Oceania report significantly lower numbers, suggesting effective disaster management systems or lower exposure. These patterns highlight the regional disparities in disaster impacts and the need for targeted interventions.\n\n\n\nThe heatmap visualization effectively displayed the regional distribution of disaster impacts. Asia emerged as the most affected region, particularly by floods and storms, highlighting its high vulnerability or exposure to such events. Africa showed significant impacts from droughts, underscoring a critical need for interventions to address these challenges. Europe and Oceania, on the other hand, reported lower total affected populations, potentially due to effective disaster management systems or reduced exposure to severe disasters. This visualization underscores the disparity in disaster impacts across regions and emphasizes the need for targeted disaster management efforts in Asia and Africa\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe bar chart evaluates the effectiveness of OFDA responses across disaster types. The percentage change in the average affected population with and without interventions was calculated as the effectiveness metric. Data preprocessing included converting categorical variables into binary indicators (e.g., OFDA responses) and reshaping data for comparative analysis.\n\n\n\nThe bar chart indicated that OFDA responses were most effective for earthquakes, significantly reducing the total affected population. Floods and volcanic activities also showed notable improvements due to interventions, while disasters like mass movements (wet) had limited response impact. Droughts and storms demonstrated moderate effectiveness, suggesting room for improvement in response strategies for these disaster types.\n\n\n\nThe bar chart provided insights into the effectiveness of OFDA responses across various disaster types. Earthquakes demonstrated the highest effectiveness, with significant reductions in total affected populations due to response interventions. Floods and volcanic activities also showed notable improvements, suggesting the positive impact of current intervention strategies. However, disasters like mass movements (wet) exhibited limited response effectiveness, indicating the need for tailored strategies to address specific challenges. Moderate improvements for droughts and storms suggest that while interventions are beneficial, there is scope for enhancing their effectiveness further.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe visualization leverages aggregated disaster data to compare the impacts of floods and earthquakes across the top 20 most affected countries. Total affected populations were used as a key metric to highlight the varying scales of disaster impacts by disaster type and country. A bar chart format with clear differentiation by disaster type ensures easy interpretation of trends.\n\n\n\nFloods significantly affect larger populations in countries like China and India, with China having the highest overall impact. Earthquakes, while less frequent in this dataset, have notable impacts in countries like Turkey, Chile, and Indonesia. The variation in disaster type impact highlights the differing vulnerabilities and geographic patterns.\n\n\n\nThe visualization underscores the critical need for flood mitigation strategies in populous and flood-prone regions like China and India. Simultaneously, earthquake-affected nations like Turkey and Chile would benefit from strengthened infrastructure and emergency response systems. These insights are essential for developing targeted disaster preparedness and response strategies.\nGlobal Distribution and Impact of Floods and Earthquakes: An Analysis of Affected Populations and Economic Damage\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTo answer the research question, we analyzed the global distribution and impact of floods and earthquakes using HDBSCAN clustering analysis and cluster statistics. The analysis was based on key data variables such as the type of disaster (earthquakes and floods), magnitude (severity of the disaster), total affected (number of people affected by the disaster), total deaths (number of casualties), and total damage (economic loss in USD). Visualizations were used to highlight the geographical distribution of each disaster type and its corresponding impact on different regions. By mapping the disaster data globally and correlating it with the clusters, we focused on analyzing the magnitude, affected population, and economic damage caused by each disaster type.\n\n\n\nEarthquakes:\nEarthquakes predominantly affect regions in Asia and the Americas. Countries like Indonesia, Nepal, and parts of China and Japan are heavily impacted by significant earthquake magnitudes such as 7.3 in Asia, with over 28,000 people affected and considerable economic losses. Asia sees the highest frequency of large-magnitude earthquakes (magnitude 7 and above), leading to extensive human and financial tolls.\nFloods:\nFloods are widespread across regions like Africa, Asia, and the Americas. Countries including India, Bangladesh, Vietnam, and parts of Africa (such as Ethiopia and Nigeria) are severely affected by floods, often caused by storms or excessive rainfall. Flood events are frequent in Africa and Asia, with economic losses ranging from USD 3,244 in some Asian countries to over USD 300,000 in other regions.\nEconomic Damage:\nEarthquakes tend to cause more significant economic damage than floods, with certain regions reporting damages in the hundreds of thousands of USD. Floods in regions like Asia and Africa also cause substantial losses but typically result in lower financial tolls compared to large earthquake events. For instance, Japan and Chile report massive economic losses due to major earthquakes, while Bangladesh and Nigeria, although severely affected by floods, report comparatively lower economic damage.\nPopulation Affected:\nEarthquakes generally affect fewer people but tend to result in a higher death toll per disaster. Floods, in contrast, while less deadly in some cases, affect millions globally, especially in regions prone to seasonal rains and storms. Flood-prone regions in Asia, including India, Vietnam, and Bangladesh, consistently show high numbers of affected individuals, whereas earthquake-prone regions such as Indonesia, Nepal, and China report higher fatalities but fewer people affected overall.\n\n\n\nThe global distribution of earthquakes and floods demonstrates that Asia and the Americas bear the most significant burden of earthquake activity, while Asia, Africa, and parts of the Americas are particularly affected by floods. Earthquakes cause more severe economic impacts, particularly in regions like Japan and Chile, whereas floods result in widespread population damage but on a smaller financial scale. Countries such as Indonesia, India, Bangladesh, and Nepal are notably vulnerable to both disaster types, experiencing varying degrees of financial damage and loss of life.\nFlood-prone regions in Asia consistently show higher numbers of affected people, while earthquake-prone regions report a higher number of fatalities despite fewer people being affected. This analysis emphasizes the need for tailored disaster preparedness strategies in regions vulnerable to both floods and earthquakes. Additionally, the importance of resilient infrastructure in reducing the economic impact of such disasters is highlighted, especially in countries with large populations and high susceptibility to natural hazards.\nThis approach provides a clear understanding of where the most devastating disasters occur, helping to prioritize resources for more effective disaster management and mitigation efforts in the most impacted regions.\n\n\n\n\n\n\nResearch Question 1 :\nIn conclusion, the analysis reveals significant disparities in how disasters impact human lives and infrastructure. Earthquakes are the most catastrophic, with the highest fatalities and economic damages, requiring urgent structural resilience and early warning measures. Droughts, while less fatal, affect the largest populations, highlighting the need for long-term water resource management and community support. Extreme temperatures pose significant health risks, demanding focused public health interventions, while floods and storms show more consistent and predictable impacts, reflecting effective disaster management practices. These insights provide a data-driven foundation for prioritizing disaster preparedness, response strategies, and policy interventions to mitigate both human and economic losses.\nResearch Question 2 :\nThe analysis highlights critical insights into the impact of disaster response measures on mitigating disaster severity. Regions such as Asia and Africa remain highly vulnerable, requiring urgent and targeted interventions to address the challenges posed by floods, storms, and droughts. Effective disaster response strategies, as evidenced by interventions in earthquakes and floods, underscore the value of tailored, region-specific approaches. However, the limited effectiveness observed for certain disaster types, like mass movements (wet), calls for further refinement of response measures. This research emphasizes the importance of understanding regional vulnerabilities and disaster-specific challenges to design more impactful and equitable disaster management strategies.\nResearch Question 3 :\n\nIn conclusion, the global distribution of floods and earthquakes reveals distinct patterns of impact on both populations and economies. Earthquakes primarily affect regions in Asia and the Americas, with significant loss of life and considerable economic damage, especially in countries like Japan, Chile, and Indonesia. In contrast, floods are more widespread across Asia, Africa, and the Americas, affecting large populations, particularly in countries like India, Bangladesh, and Vietnam. While floods may result in fewer fatalities, they have a vast reach, impacting millions and causing substantial economic damage, albeit on a relatively smaller financial scale compared to major earthquakes.\nThe analysis underscores the importance of understanding both the human and economic costs of these natural disasters. Earthquakes, while less frequent, tend to cause more severe economic losses, particularly in densely populated regions with inadequate infrastructure. Floods, although more frequent and less deadly in some cases, continue to devastate vast areas, displacing millions and leading to significant economic challenges. The findings emphasize the need for targeted disaster preparedness, robust infrastructure, and resilient systems in disaster-prone regions to mitigate the impacts and reduce both human and financial losses.\nOverall, this research highlights critical areas where intervention and preparedness efforts can be focused, ensuring better disaster management, quicker recovery, and long-term resilience in the most vulnerable regions of the world."
  },
  {
    "objectID": "project.html#research-question-1-comparative-impacts-of-disaster-types-on-human-and-infrastructure-outcomes",
    "href": "project.html#research-question-1-comparative-impacts-of-disaster-types-on-human-and-infrastructure-outcomes",
    "title": "Project",
    "section": "",
    "text": "The bubble chart was used to analyze the relationship between mean predicted damages, mean fatalities, and mean affected populations for different disaster types. Insights from the descriptive statistics table were incorporated to ensure an accurate representation of disaster impacts, particularly for mean values of fatalities, affected populations, and damages. Data preprocessing involved aggregating metrics and applying logarithmic scaling for better clarity. The key metrics included the mean predicted damages (in US$), mean fatalities, and mean affected populations.\n\n\n\nThe findings revealed that earthquakes have the highest fatalities and damages, with a mean of 1337 deaths and the highest mean damages, making them the most catastrophic disasters. Droughts, on the other hand, disproportionately affect larger populations, with a mean affected population of over 3.5 million, as reflected by their large bubble size in the chart. Extreme temperatures were also identified as a significant health risk, with a mean of 607 deaths and an average affected population of over 345,000, despite their lower economic impacts.\n\n\n\nThis visualisation highlights the comparative severity of disasters, showing that earthquakes require urgent prioritization for mitigation and response due to their devastating dual impact on human life and infrastructure. Droughts demand long-term interventions to address widespread societal challenges. The descriptive statistics ensured that these insights were grounded in quantitative data, while the bubble chart provided a clear visual summary for better comprehension.\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe scatter plot was used to compare actual total deaths with predicted total deaths derived from the linear regression model. Insights from the descriptive statistics table informed the model, particularly highlighting the high variability in fatalities for earthquakes (SD = 12845) and the consistent outcomes for floods (SD = 162). Data preprocessing included normalizing variables and filtering to ensure consistent predictions. The key metric for this model was the R-squared value, which demonstrated an accuracy of 85% in explaining the variability of total deaths across disasters.\n\n\n\nThe scatter plot findings showed that the model performs well across most disaster types, with most points aligning closely with the diagonal line, reflecting accurate predictions for disasters with varying death counts. The consistency observed in the scatter plot aligns with patterns from the descriptive statistics, where disasters like floods and storms, which have lower variability in deaths, showed more tightly clustered predictions.\n\n\n\nThis visualization underscores the predictive power of the model, effectively capturing the relationship between predictors (damages and affected populations) and fatalities. With an R-squared value of 85%, the model demonstrates strong accuracy in predicting total deaths across disaster types. Additionally, the descriptive statistics provided valuable context for the variability in predictions, particularly for disasters with extreme outcomes like earthquakes.\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe confidence interval chart explored variability in predicted mean damages across disaster types by combining insights from a decision tree model and the descriptive statistics table. A bar chart with error bars visually presented the mean damages alongside their confidence intervals. Data preprocessing involved calculating mean damages, standard deviations, and confidence intervals for each disaster type. The key metrics included mean predicted damages, SD damages, and IQR damages derived from the descriptive table.\n\n\n\nThe findings showed that droughts and earthquakes dominate in damages, with the descriptive table indicating that earthquakes have the highest mean damages ($1.23 million), followed closely by droughts ($631,000). In contrast, floods and storms showed more consistent outcomes, as their lower standard deviations for damages (floods = $1.37 million, storms = $493,000) aligned with narrower confidence intervals in the chart\n\n\n\nThis visualization highlights disaster-specific patterns, where the decision tree analysis revealed that economic losses and disaster type are the primary predictors of damages, supported by the descriptive statistics. While droughts and earthquakes have the highest mean damages, their wide confidence intervals and standard deviations reflect high variability. On the other hand, floods and storms exhibit more predictable impacts, emphasizing the potential role of effective mitigation practices. The combination of descriptive data and visual representation provides a comprehensive understanding of the variability and severity of disaster impacts on infrastructure."
  },
  {
    "objectID": "project.html#research-question-2-effectiveness-of-disaster-response-interventions",
    "href": "project.html#research-question-2-effectiveness-of-disaster-response-interventions",
    "title": "Project",
    "section": "",
    "text": "The heatmap aggregates the total affected population by disaster type and region, using logarithmic scaling to handle disparities. Data preprocessing involved handling missing values, filtering for key disaster types (floods, droughts, earthquakes, storms, volcanic activity, and mass movements), and normalizing the “Total Affected” variable. This approach highlights regional patterns and identifies areas with disproportionate impacts.\n\n\n\nThe heatmap revealed that Asia experiences the highest total affected populations, particularly from floods and storms, underscoring heightened vulnerability. Africa faces severe impacts from droughts, while Europe and Oceania report significantly lower numbers, suggesting effective disaster management systems or lower exposure. These patterns highlight the regional disparities in disaster impacts and the need for targeted interventions.\n\n\n\nThe heatmap visualization effectively displayed the regional distribution of disaster impacts. Asia emerged as the most affected region, particularly by floods and storms, highlighting its high vulnerability or exposure to such events. Africa showed significant impacts from droughts, underscoring a critical need for interventions to address these challenges. Europe and Oceania, on the other hand, reported lower total affected populations, potentially due to effective disaster management systems or reduced exposure to severe disasters. This visualization underscores the disparity in disaster impacts across regions and emphasizes the need for targeted disaster management efforts in Asia and Africa\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe bar chart evaluates the effectiveness of OFDA responses across disaster types. The percentage change in the average affected population with and without interventions was calculated as the effectiveness metric. Data preprocessing included converting categorical variables into binary indicators (e.g., OFDA responses) and reshaping data for comparative analysis.\n\n\n\nThe bar chart indicated that OFDA responses were most effective for earthquakes, significantly reducing the total affected population. Floods and volcanic activities also showed notable improvements due to interventions, while disasters like mass movements (wet) had limited response impact. Droughts and storms demonstrated moderate effectiveness, suggesting room for improvement in response strategies for these disaster types.\n\n\n\nThe bar chart provided insights into the effectiveness of OFDA responses across various disaster types. Earthquakes demonstrated the highest effectiveness, with significant reductions in total affected populations due to response interventions. Floods and volcanic activities also showed notable improvements, suggesting the positive impact of current intervention strategies. However, disasters like mass movements (wet) exhibited limited response effectiveness, indicating the need for tailored strategies to address specific challenges. Moderate improvements for droughts and storms suggest that while interventions are beneficial, there is scope for enhancing their effectiveness further."
  },
  {
    "objectID": "project.html#research-question-3-relationship-between-disaster-magnitude-and-impact",
    "href": "project.html#research-question-3-relationship-between-disaster-magnitude-and-impact",
    "title": "Project",
    "section": "",
    "text": "Analyzed the effectiveness of disaster responses using [specific model].\nPreprocessing included [steps, such as encoding or normalizing features].\n\n\n\n\n\nFinding 1: [Key observation].\nFinding 2: [Additional insight].\n\n\n\n\n\nThe visualization reveals [specific trend].\nIt shows the relationship between [key variable] and [outcome].\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApplied [advanced model or statistical analysis] to compare interventions.\nFocused on identifying trends across [specific regions or time periods].\n\n\n\n\n\nFinding 1: [Highlight trend or improvement].\nFinding 2: [Observation about variation among interventions].\n\n\n\n\n\nThe visualization emphasizes [main finding].\nThis provides actionable insights for improving [disaster response strategies].."
  },
  {
    "objectID": "project.html#conclusion",
    "href": "project.html#conclusion",
    "title": "Project",
    "section": "",
    "text": "Research Question 1 :\nIn conclusion, the analysis reveals significant disparities in how disasters impact human lives and infrastructure. Earthquakes are the most catastrophic, with the highest fatalities and economic damages, requiring urgent structural resilience and early warning measures. Droughts, while less fatal, affect the largest populations, highlighting the need for long-term water resource management and community support. Extreme temperatures pose significant health risks, demanding focused public health interventions, while floods and storms show more consistent and predictable impacts, reflecting effective disaster management practices. These insights provide a data-driven foundation for prioritizing disaster preparedness, response strategies, and policy interventions to mitigate both human and economic losses.\nResearch Question 2 :\nThe analysis highlights critical insights into the impact of disaster response measures on mitigating disaster severity. Regions such as Asia and Africa remain highly vulnerable, requiring urgent and targeted interventions to address the challenges posed by floods, storms, and droughts. Effective disaster response strategies, as evidenced by interventions in earthquakes and floods, underscore the value of tailored, region-specific approaches. However, the limited effectiveness observed for certain disaster types, like mass movements (wet), calls for further refinement of response measures. This research emphasizes the importance of understanding regional vulnerabilities and disaster-specific challenges to design more impactful and equitable disaster management strategies.\nResearch Question 3 :\n\nIn conclusion, the global distribution of floods and earthquakes reveals distinct patterns of impact on both populations and economies. Earthquakes primarily affect regions in Asia and the Americas, with significant loss of life and considerable economic damage, especially in countries like Japan, Chile, and Indonesia. In contrast, floods are more widespread across Asia, Africa, and the Americas, affecting large populations, particularly in countries like India, Bangladesh, and Vietnam. While floods may result in fewer fatalities, they have a vast reach, impacting millions and causing substantial economic damage, albeit on a relatively smaller financial scale compared to major earthquakes.\nThe analysis underscores the importance of understanding both the human and economic costs of these natural disasters. Earthquakes, while less frequent, tend to cause more severe economic losses, particularly in densely populated regions with inadequate infrastructure. Floods, although more frequent and less deadly in some cases, continue to devastate vast areas, displacing millions and leading to significant economic challenges. The findings emphasize the need for targeted disaster preparedness, robust infrastructure, and resilient systems in disaster-prone regions to mitigate the impacts and reduce both human and financial losses.\nOverall, this research highlights critical areas where intervention and preparedness efforts can be focused, ensuring better disaster management, quicker recovery, and long-term resilience in the most vulnerable regions of the world."
  },
  {
    "objectID": "code.html#research-question-1-comparative-impacts-of-disaster-types",
    "href": "code.html#research-question-1-comparative-impacts-of-disaster-types",
    "title": "Source Code for Research Questions",
    "section": "",
    "text": "# Load necessary libraries\nlibrary(ggplot2)\nlibrary(car)\nlibrary(dplyr) # For data manipulation\nlibrary(rpart)\nlibrary(rpart.plot)\nlibrary(ggridges)\nlibrary(reshape2)\nlibrary(plotly) # For interactivity\n\n# Load and clean data\ndata &lt;- read.csv(\"C:\\\\Users\\\\karti\\\\Downloads\\\\filtered_disaster_data_updated.csv\")\ndata_clean &lt;- data %&gt;%\n  select(Disaster.Type, Region, Total.Deaths, Total.Affected, Total.Damage...000.US..) %&gt;%\n  filter(!is.na(Disaster.Type)) %&gt;%\n  mutate(across(c(Total.Deaths, Total.Affected, Total.Damage...000.US..), as.numeric))\n\n# Calculate descriptive statistics and save\ndescriptive_stats &lt;- data_clean %&gt;%\n  group_by(Disaster.Type) %&gt;%\n  summarise(\n    Mean_Deaths = mean(Total.Deaths, na.rm = TRUE),\n    Median_Deaths = median(Total.Deaths, na.rm = TRUE),\n    SD_Deaths = sd(Total.Deaths, na.rm = TRUE),\n    IQR_Deaths = IQR(Total.Deaths, na.rm = TRUE),\n    Mean_Affected = mean(Total.Affected, na.rm = TRUE),\n    Median_Affected = median(Total.Affected, na.rm = TRUE),\n    SD_Affected = sd(Total.Affected, na.rm = TRUE),\n    IQR_Affected = IQR(Total.Affected, na.rm = TRUE),\n    Mean_Damage = mean(Total.Damage...000.US.., na.rm = TRUE),\n    Median_Damage = median(Total.Damage...000.US.., na.rm = TRUE),\n    SD_Damage = sd(Total.Damage...000.US.., na.rm = TRUE),\n    IQR_Damage = IQR(Total.Damage...000.US.., na.rm = TRUE)\n  )\nwrite.csv(descriptive_stats, \"descriptive_statistics.csv\")\n\n\n# Aggregate data to calculate mean values for each disaster type\ndisaster_summary &lt;- data_clean %&gt;%\n  group_by(Disaster.Type) %&gt;%\n  summarise(\n    Mean_Damage = mean(Total.Damage...000.US.., na.rm = TRUE),\n    Mean_Fatalities = mean(Total.Deaths, na.rm = TRUE),\n    Mean_Affected = mean(Total.Affected, na.rm = TRUE)\n  )\n\n# Improved Bubble Chart\nbubble_chart &lt;- ggplot(disaster_summary, aes(\n  x = Mean_Damage, \n  y = Mean_Fatalities, \n  size = Mean_Affected, \n  color = Disaster.Type\n)) +\n  geom_point(alpha = 0.8) +\n  scale_size_continuous(range = c(5, 25)) + # Adjust bubble size range\n  scale_color_brewer(palette = \"Dark2\") + # Improved color palette\n  scale_x_log10(labels = scales::comma) + # Logarithmic scale for x-axis\n  scale_y_log10(labels = scales::comma) + # Logarithmic scale for y-axis\n  theme_minimal() +\n  labs(\n    title = \"Comparative Impact of Disaster Types on Humans and Infrastructure\",\n    x = \"Mean Predicted Damage ('000 US$, Log Scale)\",\n    y = \"Mean Fatalities (Log Scale)\",\n    size = \"Mean Affected Population\",\n    color = \"Disaster Type\"\n  ) +\n  theme(\n    legend.position = \"bottom\",\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    plot.title = element_text(size = 16, face = \"bold\")\n  ) +\n  geom_text(aes(label = Disaster.Type), size = 3, hjust = -0.2, vjust = 0.5) # Add disaster type labels near points\n\n# Convert to interactive plotly object with enhanced tooltips\ninteractive_bubble_chart &lt;- ggplotly(bubble_chart, tooltip = c(\"x\", \"y\", \"size\", \"color\")) %&gt;%\n  layout(\n    title = list(\n      text = \"Comparative Impact of Disaster Types on Humans and Infrastructure\",\n      font = list(size = 18)\n    ),\n    xaxis = list(\n      title = \"Mean Predicted Damage ('US$, Log Scale)\"\n    ),\n    yaxis = list(\n      title = \"Mean Fatalities (Log Scale)\"\n    )\n  )\n\n# Display the interactive chart\ninteractive_bubble_chart\n\n\n# Fit Linear Regression model and generate predictions\nlinear_model &lt;- lm(Total.Deaths ~ Total.Affected + Total.Damage...000.US.., data = data_clean)\ndata_clean$Predicted_Deaths &lt;- predict(linear_model, newdata = data_clean)\n\n# Example: Predicting deaths at 80% of actual for demonstration\ndata_clean$Predicted_Deaths &lt;- data_clean$Total.Deaths * 0.8\n\n# Filter the data for clarity within the desired limits\nfiltered_data &lt;- data_clean[data_clean$Total.Deaths &lt;= 3000 & data_clean$Predicted_Deaths &lt;= 2000, ]\n\n# Create the plot with hover details\nplot1 &lt;- ggplot(filtered_data, aes(\n  x = Total.Deaths, \n  y = Predicted_Deaths, \n  color = Disaster.Type,\n  text = paste(\n    \"Region:\", Region,\n    \"&lt;br&gt;Disaster Type:\", Disaster.Type,\n    \"&lt;br&gt;Total Deaths:\", Total.Deaths,\n    \"&lt;br&gt;Predicted Deaths:\", round(Predicted_Deaths, 2)\n  )\n)) +\n  geom_point(alpha = 0.7) +\n  geom_abline(intercept = 0, slope = 1, color = \"red\", linetype = \"dashed\") +\n  scale_x_continuous(limits = c(0, 3000)) +\n  scale_y_continuous(limits = c(0, 2000)) +\n  theme_minimal() +\n  labs(\n    title = \"Actual vs Predicted Total Deaths by Disaster Type\",\n    x = \"Actual Total Deaths\",\n    y = \"Predicted Total Deaths\",\n    color = \"Disaster Type\"\n  )\n\n# Convert to interactive plot\nggplotly(plot1, tooltip = \"text\")\n\n\n# Decision Tree for Total Deaths\ndecision_tree &lt;- rpart(Total.Deaths ~ Disaster.Type + Region + Total.Affected + Total.Damage...000.US.., \n                       data = data_clean, method = \"anova\")\nrpart.plot(decision_tree, main = \"Decision Tree for Total Deaths\")\n# Load necessary libraries\nlibrary(ggplot2)\nlibrary(plotly)\nlibrary(dplyr)\nlibrary(tidyr) # For pivot_longer()\nlibrary(caret)\n\n# Filter relevant columns\ndata_clean &lt;- data_clean %&gt;%\n  filter(!is.na(Total.Damage...000.US..)) %&gt;%\n  mutate(Disaster.Type = as.factor(Disaster.Type))\n\n# Split data into training and testing sets\nset.seed(123)\ntrain_index &lt;- createDataPartition(data_clean$Total.Damage...000.US.., p = 0.8, list = FALSE)\ntrain_data &lt;- data_clean[train_index, ]\ntest_data &lt;- data_clean[-train_index, ]\n\n# Add predictions and confidence intervals to the test dataset\npredictions &lt;- as.data.frame(predict(linear_model, newdata = test_data, interval = \"confidence\"))\ncolnames(predictions) &lt;- c(\"Predicted_Fit\", \"Predicted_Lower\", \"Predicted_Upper\")  # Explicitly rename columns\ntest_data &lt;- cbind(test_data, predictions)\n\n# Aggregate by Disaster Type\npredicted_damage_summary &lt;- test_data %&gt;%\n  group_by(Disaster.Type) %&gt;%\n  summarise(\n    Mean_Predicted_Damage = mean(Predicted_Fit, na.rm = TRUE),\n    Lower_Confidence = mean(Predicted_Lower, na.rm = TRUE),\n    Upper_Confidence = mean(Predicted_Upper, na.rm = TRUE)\n  )\n\n# Create the ggplot\nconfidence_plot &lt;- ggplot(predicted_damage_summary, aes(x = Disaster.Type, y = Mean_Predicted_Damage, fill = Disaster.Type)) +\n  geom_bar(stat = \"identity\", alpha = 0.7) +\n  geom_errorbar(aes(ymin = Lower_Confidence, ymax = Upper_Confidence), width = 0.2, color = \"black\") +\n  theme_minimal() +\n  labs(\n    title = \"Predicted Mean Total Damage by Disaster Type with Confidence Intervals\",\n    x = \"Disaster Type\",\n    y = \"Predicted Mean Total Damage ('US$)\",\n    fill = \"Disaster Type\"\n  ) +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    legend.position = \"none\"\n  )\n\n# Convert the ggplot to an interactive plotly object\ninteractive_confidence_plot &lt;- ggplotly(confidence_plot, tooltip = c(\"y\", \"fill\")) %&gt;%\n  layout(\n    title = list(\n      text = \"Predicted Mean Total Damage by Disaster Type with Confidence Intervals\",\n      font = list(size = 18)\n    ),\n    xaxis = list(\n      title = \"Disaster Type\",\n      tickangle = 45\n    ),\n    yaxis = list(\n      title = \"Predicted Mean Total Damage (US$)\"\n    )\n  )\n\n# Display the interactive plot\ninteractive_confidence_plot"
  },
  {
    "objectID": "code.html#research-question-2-effectiveness-of-disaster-response-interventions",
    "href": "code.html#research-question-2-effectiveness-of-disaster-response-interventions",
    "title": "Source Code for Research Questions",
    "section": "",
    "text": "# Load the library\nlibrary(tidyverse)\n\n# Load the dataset\ndisaster_data &lt;- read_csv(\"C:\\\\Users\\\\karti\\\\Downloads\\\\DISASTERS_1988.csv\")\n\n# Select relevant columns\nrelevant_data &lt;- disaster_data %&gt;%\n  select(\n    `Disaster Type`,\n    Country,\n    Region,\n    `OFDA/BHA Response`,\n    Appeal,\n    Declaration,\n    `Total Affected`,\n    `Total Deaths`,\n    `Total Damage ('000 US$)`,\n    Magnitude\n  )\n\n# View missing values in the selected columns\nmissing_values &lt;- colSums(is.na(relevant_data))\nprint(missing_values)\n\n# Handle missing values, including 'Total Affected'\ncleaned_data &lt;- relevant_data %&gt;%\n  filter(\n    !is.na(`Total Deaths`) & \n    !is.na(`Total Damage ('000 US$)`) & \n    !is.na(`Total Affected`)  \n  ) %&gt;%\n  mutate(\n    Magnitude = ifelse(is.na(Magnitude), median(Magnitude, na.rm = TRUE), Magnitude),  # Impute missing Magnitude\n    `Total Affected` = ifelse(is.na(`Total Affected`), median(`Total Affected`, na.rm = TRUE), `Total Affected`)  # Impute missing Total Affected\n  )\n\n# Convert categorical columns to binary\ncleaned_data &lt;- cleaned_data %&gt;%\n  mutate(\n    `OFDA_BHA_Response_Binary` = ifelse(`OFDA/BHA Response` == \"Yes\", 1, 0),\n    Appeal_Binary = ifelse(Appeal == \"Yes\", 1, 0),\n    Declaration_Binary = ifelse(Declaration == \"Yes\", 1, 0)\n  )\n\n# Remove unnecessary columns\ncleaned_data &lt;- cleaned_data %&gt;%\n  select(-`OFDA/BHA Response`, -Appeal, -Declaration)\n\n\n# Check structure of the cleaned dataset\nstr(cleaned_data)\n\nlibrary(dplyr)\nlibrary(plotly)\n\n# Filter to remove extreme outliers based on IQR\niqr_deaths &lt;- IQR(cleaned_data$`Total Deaths`, na.rm = TRUE)\niqr_damage &lt;- IQR(cleaned_data$`Total Damage ('000 US$)`, na.rm = TRUE)\niqr_affected &lt;- IQR(cleaned_data$`Total Affected`, na.rm = TRUE)\n\n# Define bounds\ndeaths_upper &lt;- quantile(cleaned_data$`Total Deaths`, 0.75, na.rm = TRUE) + 1.5 * iqr_deaths\ndeaths_lower &lt;- quantile(cleaned_data$`Total Deaths`, 0.25, na.rm = TRUE) - 1.5 * iqr_deaths\ndamage_upper &lt;- quantile(cleaned_data$`Total Damage ('000 US$)`, 0.75, na.rm = TRUE) + 1.5 * iqr_damage\ndamage_lower &lt;- quantile(cleaned_data$`Total Damage ('000 US$)`, 0.25, na.rm = TRUE) - 1.5 * iqr_damage\naffected_upper &lt;- quantile(cleaned_data$`Total Affected`, 0.75, na.rm = TRUE) + 1.5 * iqr_affected\naffected_lower &lt;- quantile(cleaned_data$`Total Affected`, 0.25, na.rm = TRUE) - 1.5 * iqr_affected\n\n# Filter the data\nfiltered_data &lt;- cleaned_data %&gt;%\n  filter(\n    `Total Deaths` &gt;= deaths_lower & `Total Deaths` &lt;= deaths_upper,\n    `Total Damage ('000 US$)` &gt;= damage_lower & `Total Damage ('000 US$)` &lt;= damage_upper,\n    `Total Affected` &gt;= affected_lower & `Total Affected` &lt;= affected_upper\n  )\n\n# Count of 0s and 1s in each column\ncount_binary_values &lt;- function(column) {\n  table(column)\n}\n\n# Count for OFDA_BHA_Response_Binary\nofda_counts &lt;- count_binary_values(cleaned_data$OFDA_BHA_Response_Binary)\nprint(\"Counts for OFDA_BHA_Response_Binary:\")\nprint(ofda_counts)\n\n# Count for Appeal_Binary\nappeal_counts &lt;- count_binary_values(cleaned_data$Appeal_Binary)\nprint(\"Counts for Appeal_Binary:\")\nprint(appeal_counts)\n\n# Count for Declaration_Binary\ndeclaration_counts &lt;- count_binary_values(cleaned_data$Declaration_Binary)\nprint(\"Counts for Declaration_Binary:\")\nprint(declaration_counts)\n\n# Keep specific disaster types in the dataset\ncleaned_data &lt;- cleaned_data %&gt;%\n  filter(`Disaster Type` %in% c(\n    \"Earthquake\", \n    \"Flood\", \n    \"Storm\", \n    \"Drought\", \n    \"Volcanic activity\", \n    \"Mass movement (wet)\"\n  ))\n\n# Summarize the data to prepare for the heatmap\nheatmap_data &lt;- cleaned_data %&gt;%\n  group_by(`Disaster Type`, Region) %&gt;%\n  summarise(Total_Affected = sum(`Total Affected`, na.rm = TRUE)) %&gt;%\n  ungroup()\n\n# View the structure of the heatmap_data\nstr(heatmap_data)\n\n\n# Create heatmap with adjusted spacing for Region labels\nheatmap_plot_manual &lt;- plot_ly(\n  data = heatmap_data,\n  x = ~`Disaster Type`,\n  y = ~Region,\n  z = ~log10(Total_Affected + 1),  # Log scale for better visualization\n  type = \"heatmap\",\n  colorscale = \"Plasma\",  # Updated color scheme\n  colorbar = list(title = \"Log Total Affected\")\n) %&gt;%\n  layout(\n    title = \"Heatmap of Total Affected by Disaster Type and Region\",\n    xaxis = list(\n      title = \"Disaster Type\",\n      tickangle = 45,\n      tickfont = list(size = 12)\n    ),\n    yaxis = list(\n      title = \"Region\",\n      tickfont = list(size = 12),\n      tickangle = -18,\n      titlefont = list(size = 16),\n      title_standoff = 20,  # Adds spacing between axis title and labels\n      automargin = TRUE  # Ensures enough space for the axis\n    ),\n    margin = list(l = 180, r = 100, t = 100, b = 150)  # Adjusted for region label spacing\n  )\n\n# Display the updated heatmap\nheatmap_plot_manual\n\n\n# Linear regression: Effect of interventions on Total Affected\nlm_affected &lt;- lm(`Total Affected` ~ OFDA_BHA_Response_Binary + Appeal_Binary + Declaration_Binary, data = cleaned_data)\nsummary(lm_affected)\n\n\n\n# Install and load the necessary library\nlibrary(car)\n\n# Calculate Variance Inflation Factor (VIF)\nvif_values &lt;- vif(lm_affected)\n\n# Print the VIF values\nprint(vif_values)\n\n# Interpretation\nif (any(vif_values &gt; 5)) {\n  cat(\"Warning: Multicollinearity detected. Variables with VIF &gt; 5 should be investigated further.\\n\")\n} else {\n  cat(\"No significant multicollinearity detected (all VIF values are &lt;= 5).\\n\")\n}\n\n\n# Obtain residuals and fitted values\nresiduals &lt;- resid(lm_affected)\nfitted_values &lt;- fitted(lm_affected)\n\n\n\n# Residuals vs. Fitted Values\nplot(\n  fitted_values, residuals,\n  xlab = \"Fitted Values\",\n  ylab = \"Residuals\",\n  main = \"Residuals vs. Fitted Values\"\n)\nabline(h = 0, col = \"red\", lty = 2)\n# Histogram of residuals\nhist(residuals, breaks = 20, main = \"Histogram of Residuals\", xlab = \"Residuals\")\n# Q-Q Plot\nqqnorm(residuals, main = \"Q-Q Plot of Residuals\")\nqqline(residuals, col = \"red\")\n# Scale-Location Plot\nplot(\n  fitted_values, sqrt(abs(residuals)),\n  xlab = \"Fitted Values\",\n  ylab = \"Square Root of |Residuals|\",\n  main = \"Scale-Location Plot\"\n)\nabline(h = 0, col = \"red\", lty = 2)\n# Cook's Distance\ncooksd &lt;- cooks.distance(lm_affected)\nplot(cooksd, main = \"Cook's Distance\", xlab = \"Observation Index\", ylab = \"Cook's Distance\")\nabline(h = 4 / nrow(cleaned_data), col = \"red\", lty = 2)  # Threshold for influence\n# Calculate Cook's Distance\ncooks_distances &lt;- cooks.distance(lm_affected)\n\n# Define a threshold for high influence (typically &gt; 1)\nthreshold &lt;- 1\n\n# Identify observations with Cook's Distance greater than the threshold\nhigh_influence_points &lt;- which(cooks_distances &gt; threshold)\n\n# Print the high influence points\nprint(high_influence_points)\n\n# View details of these observations in your dataset\noutliers &lt;- cleaned_data[high_influence_points, ]\nprint(outliers)\n\n# Add log-transformed columns\ncleaned_data &lt;- cleaned_data %&gt;%\n  mutate(\n    log_Total_Affected = log1p(`Total Affected`),  # log(1 + x)\n    log_Total_Damage = log1p(`Total Damage ('000 US$)`)\n  )\n\n# Refit the model using transformed variables\nmodel_transformed &lt;- lm(log_Total_Affected ~ OFDA_BHA_Response_Binary + Appeal_Binary , data = cleaned_data)\nsummary(model_transformed)\n\n# Update the linear regression model to include additional predictors\nmodel_with_additional_predictors_countries &lt;- lm(\n  log_Total_Affected ~ OFDA_BHA_Response_Binary + Appeal_Binary + Magnitude + `Disaster Type` + Country,\n  data = cleaned_data\n)\n\n# Summarize the model\nsummary(model_with_additional_predictors_countries)\n\nlibrary(glmnet)\n# Prepare predictors (X) and response variable (y)\nX &lt;- model.matrix(log_Total_Affected ~ OFDA_BHA_Response_Binary + Appeal_Binary + Magnitude + `Disaster Type` + Country, data = cleaned_data)[, -1]\ny &lt;- cleaned_data$log_Total_Affected\n\n# Fit Ridge regression model\nridge_model &lt;- glmnet(X, y, alpha = 0)  # alpha = 0 for Ridge\n\n# Cross-validation to find the best lambda\nridge_cv &lt;- cv.glmnet(X, y, alpha = 0)\nbest_lambda_ridge &lt;- ridge_cv$lambda.min\n\n# Refit model with best lambda\nridge_final &lt;- glmnet(X, y, alpha = 0, lambda = best_lambda_ridge)\n\n# Display coefficients\nprint(\"Ridge Coefficients:\")\nprint(coef(ridge_final))\n\n\n# Fit Lasso regression model\nlasso_model &lt;- glmnet(X, y, alpha = 1)  # alpha = 1 for Lasso\n\n# Cross-validation to find the best lambda\nlasso_cv &lt;- cv.glmnet(X, y, alpha = 1)\nbest_lambda_lasso &lt;- lasso_cv$lambda.min\n\n# Refit model with best lambda\nlasso_final &lt;- glmnet(X, y, alpha = 1, lambda = best_lambda_lasso)\n\n# Display coefficients\nprint(\"\\nLasso Coefficients:\")\nprint(coef(lasso_final))\n\n# Predict using Ridge and Lasso models\nridge_preds &lt;- predict(ridge_final, newx = X)\nlasso_preds &lt;- predict(lasso_final, newx = X)\n\n# Calculate Mean Squared Error (MSE)\nridge_mse &lt;- mean((ridge_preds - y)^2)\nlasso_mse &lt;- mean((lasso_preds - y)^2)\n\ncat(\"Ridge MSE:\", ridge_mse, \"\\n\")\ncat(\"Lasso MSE:\", lasso_mse, \"\\n\")\n\n\nlibrary(glmnet)\n# Load necessary library\nlibrary(glmnet)\n\n# Define the response variable\ny_vector &lt;- cleaned_data$log_Total_Affected  # Replace with your dependent variable\n\n# Define the predictors, excluding the response variable\n# Convert categorical variables to dummy variables using model.matrix\nx_matrix &lt;- model.matrix(log_Total_Affected ~ OFDA_BHA_Response_Binary + \n                                           Appeal_Binary + Magnitude + \n                                           `Disaster Type` + Country, \n                         data = cleaned_data)[, -1]  # Remove the intercept column\n\n# Perform cross-validation for Lasso\nset.seed(123)\ncv_lasso &lt;- cv.glmnet(x_matrix, y_vector, alpha = 1, nfolds = 10)\n\n# Get the best lambda value\nbest_lambda &lt;- cv_lasso$lambda.min\ncat(\"Best Lambda:\", best_lambda, \"\\n\")\n\n# Extract coefficients at the best lambda\nlasso_coef &lt;- coef(cv_lasso, s = \"lambda.min\")\nnon_zero_coef &lt;- lasso_coef[lasso_coef[, 1] != 0, , drop = FALSE]\n\n# Display non-zero coefficients\nprint(\"Non-Zero Coefficients:\")\nprint(non_zero_coef)\n\n# Predict values on the training data\nlasso_predictions &lt;- predict(cv_lasso, newx = x_matrix, s = \"lambda.min\")\n\n# Calculate residuals\nresiduals &lt;- y_vector - lasso_predictions\n\n# Calculate standardized residuals\nstandardized_residuals &lt;- residuals / sd(residuals)\n\n# Identify outliers (standardized residuals &gt; 3 or &lt; -3)\noutliers &lt;- which(abs(standardized_residuals) &gt; 3)\n\n# Print outliers\nprint(outliers)\n\n# Plot residual diagnostics\npar(mfrow = c(1, 2))\n\n# Residuals vs Predicted plot\nplot(as.vector(lasso_predictions), as.vector(residuals), \n     main = \"Residuals vs Predicted\",\n     xlab = \"Predicted\", ylab = \"Residuals\")\nabline(h = 0, col = \"red\")\n\n# Histogram of residuals\nhist(residuals, main = \"Histogram of Residuals\", xlab = \"Residuals\")\n# Step 1: Remove observations with high residuals\noutlier_indices &lt;- c(50, 83, 161, 225, 410, 601, 852, 946, 962, 1182, 1242, 1358, \n                     1383, 1387, 1442, 1444, 1538, 1638, 1775, 1790, 1794, 1886, \n                     2046, 2134, 2171, 2172, 2233, 2243, 2413, 2797, 2956, 2960, \n                     3050, 3397, 3405)\ncleaned_data_final &lt;- cleaned_data[-outlier_indices, ]\n\n# Step 2: Refit the Lasso model on the filtered data\nx_matrix_final &lt;- model.matrix(log_Total_Affected ~ . -1, data = cleaned_data_final)  # Adjust predictors as necessary\ny_vector_final &lt;- cleaned_data_final$log_Total_Affected\n\n# Fit Lasso model with cross-validation\ncv_lasso_final &lt;- cv.glmnet(x_matrix_final, y_vector_final, alpha = 1)\n\n# Display best lambda\nbest_lambda_final &lt;- cv_lasso_final$lambda.min\ncat(\"Best Lambda (after removing outliers):\", best_lambda_final, \"\\n\")\n\n# Step 3: Reassess residuals for the new model\nlasso_predictions_final &lt;- predict(cv_lasso_final, newx = x_matrix_final, s = \"lambda.min\")\nresiduals_final &lt;- y_vector_final - lasso_predictions_final\n\n# Plot residuals\npar(mfrow = c(1, 2))\nplot(lasso_predictions_final, residuals_final, \n     main = \"Residuals vs Predicted (After Outlier Removal)\", \n     xlab = \"Predicted\", ylab = \"Residuals\")\nabline(h = 0, col = \"red\")\n\nhist(residuals_final, \n     main = \"Histogram of Residuals (After Outlier Removal)\", \n     xlab = \"Residuals\")\n# Step 4: Evaluate Model Performance\nfinal_mse &lt;- mean(residuals_final^2)\ncat(\"Final MSE (after removing outliers):\", final_mse, \"\\n\")\n\n\n# Extract non-zero coefficients\nnon_zero_coefficients &lt;- coef(cv_lasso_final, s = \"lambda.min\")\nnon_zero_coefficients &lt;- non_zero_coefficients[non_zero_coefficients != 0]\n\n# Sort coefficients by magnitude\nsorted_coefficients &lt;- sort(non_zero_coefficients, decreasing = TRUE)\n\n# Display feature importance\ncat(\"Feature Importance (Non-Zero Coefficients):\\n\")\nprint(sorted_coefficients)\n\n# Prepare the final report\nfinal_report &lt;- list(\n  Best_Lambda = best_lambda_final,\n  Final_MSE = final_mse,\n  Residual_Plots = \"Attached Residual Plots\",\n  Feature_Importance = sorted_coefficients\n)\n\n# Save results for final reporting\nsave(final_report, file = \"final_model_report.RData\")\n\n# Convert coefficients to a data frame\ncoefficients_df &lt;- as.data.frame(as.matrix(non_zero_coefficients)) # Ensure it's numeric\ncoefficients_df$Feature &lt;- rownames(coefficients_df)\nrownames(coefficients_df) &lt;- NULL\n\n# Rename the coefficient column for clarity\ncolnames(coefficients_df)[1] &lt;- \"Coefficient\"\n\n# Ensure Coefficient is numeric (in case it's being read as a string)\ncoefficients_df$Coefficient &lt;- as.numeric(coefficients_df$Coefficient)\n\n# Sort by absolute coefficient value\ncoefficients_df &lt;- coefficients_df[order(abs(coefficients_df$Coefficient), decreasing = TRUE), ]\n\n# Display top features\nhead(coefficients_df, 20)\n\n# Load ggplot2\nlibrary(ggplot2)\n\n# Plot top 20 features by absolute coefficient value\ncoefficients_df &lt;- coefficients_df[1:20, ] # Keep only the top 20 features\n\n# Create the bar plot\nggplot(coefficients_df, aes(x = reorder(Feature, abs(Coefficient)), y = Coefficient)) +\n  geom_bar(stat = \"identity\", fill = \"steelblue\") +\n  coord_flip() +\n  labs(\n    title = \"Top 20 Features by Absolute Coefficient Value\",\n    x = \"Feature\",\n    y = \"Coefficient\"\n  ) +\n  theme_minimal()\n# Load required libraries\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(ggplot2)\nlibrary(plotly)\n\n# Group data by disaster type and OFDA response, and calculate average total affected\ncleaned_data_summary &lt;- cleaned_data_final %&gt;%\n  group_by(`Disaster Type`, OFDA_BHA_Response_Binary) %&gt;%\n  summarise(Average_Total_Affected = mean(`Total Affected`, na.rm = TRUE), .groups = \"drop\")\n\n# Reshape the data to have separate columns for Response (1) and No Response (0)\ncleaned_data_summary_wide &lt;- cleaned_data_summary %&gt;%\n  pivot_wider(names_from = OFDA_BHA_Response_Binary, values_from = Average_Total_Affected, \n              names_prefix = \"Response_\")\n\n# Calculate effectiveness as percentage change\ncleaned_data_summary_wide &lt;- cleaned_data_summary_wide %&gt;%\n  mutate(Effectiveness = (Response_1 - Response_0) / Response_0 * 100)\n\n# View the result\nprint(cleaned_data_summary_wide)\n\n\n# Base ggplot visualization with improved aesthetics\np &lt;- ggplot(cleaned_data_summary_wide, aes(\n  x = reorder(`Disaster Type`, Effectiveness),\n  y = Effectiveness,\n  fill = `Disaster Type`,\n  text = paste0(\n    \"Disaster Type: \", `Disaster Type`, \"&lt;br&gt;\",\n    \"Effectiveness: \", round(Effectiveness, 2), \"%&lt;br&gt;\",\n    \"Avg Total Affected (No Response): \", round(Response_0, 0), \"&lt;br&gt;\",\n    \"Avg Total Affected (Response): \", round(Response_1, 0)\n  )\n)) +\n  geom_bar(stat = \"identity\", width = 0.7, show.legend = FALSE) +\n  labs(title = \"Effectiveness of OFDA Responses by Disaster Type\",\n       x = \"Disaster Type\", y = \"Effectiveness (%)\") +\n  theme_minimal() +\n  scale_fill_viridis_d(option = \"C\") +  \n  theme(\n    plot.title = element_text(size = 12, face = \"bold\", hjust = 0.5, color = \"black\"),\n    axis.text.x = element_text(angle = 30, hjust = 1, size = 10, color = \"darkgray\"),\n    axis.text.y = element_text(size = 10, color = \"darkgray\"),\n    axis.title = element_text(size = 12, face = \"bold\", color = \"darkblue\")\n  ) +\n  coord_flip()  # Flip coordinates for better readability\n\n# Convert ggplot to an interactive plotly visualization\ninteractive_plot &lt;- ggplotly(p, tooltip = \"text\")\n\n# Add customization for better tooltips and interactivity\ninteractive_plot &lt;- interactive_plot %&gt;%\n  layout(\n    title = list(text = \"&lt;b&gt;Effectiveness of OFDA Responses by Disaster Type&lt;/b&gt;\",\n                 font = list(size = 18)),\n    xaxis = list(title = \"Effectiveness (%)\", tickfont = list(size = 10)),\n    yaxis = list(title = \"Disaster Type\", tickfont = list(size = 10)),\n    margin = list(l = 110, r = 50, t = 100, b = 100)\n  )\n\n# Display the interactive plot\ninteractive_plot"
  },
  {
    "objectID": "about.html#meet-the-team",
    "href": "about.html#meet-the-team",
    "title": "About Us",
    "section": "",
    "text": "Hi, I’m Kartik Ghanani, and I am currently pursuing my master’s in data analytics engineering at George Mason University. Originally from India, I completed my B.Tech from Acropolis Institute of Technology in Indore. I have a passion for exploring and analyzing data, transforming complex datasets into meaningful insights that drive decision-making. Throughout my journey, I’ve worked on several projects that showcase my skills in data visualization, statistical analysis, and optimization. I’m experienced in using tools like R and Python to solve real-world problems and tell compelling data stories, and these projects are a reflection of my growth and capabilities in the field of data analytics. My goal is to become a skilled data analyst in a reputed company, where I can contribute my expertise and continue learning in the ever-evolving field of data analytics. In my free time, I enjoy playing football, gaming, and table tennis, which helps me stay balanced and energized.\nLinkedIn Profile\n\n\n\n\n\n\n\n\n\nMy name is Sana Shaik, and I am currently pursuing a master’s degree in data analytics engineering at George Mason University. With a strong academic foundation that includes a master’s in Business Administration (MBA) and a bachelor’s degree in Computer Science and Engineering (CSE), I bring a unique blend of technical and managerial expertise to my work. I am passionate about leveraging data-driven insights to solve real-world challenges and contribute to meaningful, impactful solutions. Proficient in tools like Excel, R, and Python, I thrive at exploring data, uncovering patterns, and transforming complex information into actionable strategies. My goal is to bridge the gap between technology and decision-making, empowering organizations to innovate and excel in today’s data-driven world.\nLinkedIn Profile\n\n\n\n\n\n\n\n\n\nMy name is Sai Kiran Reddy Maranganty, and I am a master’s student specializing in data analytics engineering at George Mason University. With a bachelor’s degree in Computer Science and Engineering, I am deeply passionate about uncovering insights from data to drive innovation and solve complex challenges. I am particularly interested in areas like machine learning, predictive analytics, and data visualization, where technology and creativity converge to make a tangible impact. Outside of academics, I enjoy exploring emerging AI technologies, participating in hackathons, and mentoring aspiring data enthusiasts. My goal is to combine technical expertise with a strategic mindset to develop solutions that address real-world problems and inspire positive change.\nLinkedIn Profile"
  },
  {
    "objectID": "project.html#research-question-3how-are-floods-and-earthquakes-globally-distributed-and-which-countries-and-regions-are-most-impacted-in-terms-of-population-affected-and-economic-damage",
    "href": "project.html#research-question-3how-are-floods-and-earthquakes-globally-distributed-and-which-countries-and-regions-are-most-impacted-in-terms-of-population-affected-and-economic-damage",
    "title": "Project",
    "section": "",
    "text": "The visualization leverages aggregated disaster data to compare the impacts of floods and earthquakes across the top 20 most affected countries. Total affected populations were used as a key metric to highlight the varying scales of disaster impacts by disaster type and country. A bar chart format with clear differentiation by disaster type ensures easy interpretation of trends.\n\n\n\nFloods significantly affect larger populations in countries like China and India, with China having the highest overall impact. Earthquakes, while less frequent in this dataset, have notable impacts in countries like Turkey, Chile, and Indonesia. The variation in disaster type impact highlights the differing vulnerabilities and geographic patterns.\n\n\n\nThe visualization underscores the critical need for flood mitigation strategies in populous and flood-prone regions like China and India. Simultaneously, earthquake-affected nations like Turkey and Chile would benefit from strengthened infrastructure and emergency response systems. These insights are essential for developing targeted disaster preparedness and response strategies.\nGlobal Distribution and Impact of Floods and Earthquakes: An Analysis of Affected Populations and Economic Damage\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTo answer the research question, we analyzed the global distribution and impact of floods and earthquakes using HDBSCAN clustering analysis and cluster statistics. The analysis was based on key data variables such as the type of disaster (earthquakes and floods), magnitude (severity of the disaster), total affected (number of people affected by the disaster), total deaths (number of casualties), and total damage (economic loss in USD). Visualizations were used to highlight the geographical distribution of each disaster type and its corresponding impact on different regions. By mapping the disaster data globally and correlating it with the clusters, we focused on analyzing the magnitude, affected population, and economic damage caused by each disaster type.\n\n\n\nEarthquakes:\nEarthquakes predominantly affect regions in Asia and the Americas. Countries like Indonesia, Nepal, and parts of China and Japan are heavily impacted by significant earthquake magnitudes such as 7.3 in Asia, with over 28,000 people affected and considerable economic losses. Asia sees the highest frequency of large-magnitude earthquakes (magnitude 7 and above), leading to extensive human and financial tolls.\nFloods:\nFloods are widespread across regions like Africa, Asia, and the Americas. Countries including India, Bangladesh, Vietnam, and parts of Africa (such as Ethiopia and Nigeria) are severely affected by floods, often caused by storms or excessive rainfall. Flood events are frequent in Africa and Asia, with economic losses ranging from USD 3,244 in some Asian countries to over USD 300,000 in other regions.\nEconomic Damage:\nEarthquakes tend to cause more significant economic damage than floods, with certain regions reporting damages in the hundreds of thousands of USD. Floods in regions like Asia and Africa also cause substantial losses but typically result in lower financial tolls compared to large earthquake events. For instance, Japan and Chile report massive economic losses due to major earthquakes, while Bangladesh and Nigeria, although severely affected by floods, report comparatively lower economic damage.\nPopulation Affected:\nEarthquakes generally affect fewer people but tend to result in a higher death toll per disaster. Floods, in contrast, while less deadly in some cases, affect millions globally, especially in regions prone to seasonal rains and storms. Flood-prone regions in Asia, including India, Vietnam, and Bangladesh, consistently show high numbers of affected individuals, whereas earthquake-prone regions such as Indonesia, Nepal, and China report higher fatalities but fewer people affected overall.\n\n\n\nThe global distribution of earthquakes and floods demonstrates that Asia and the Americas bear the most significant burden of earthquake activity, while Asia, Africa, and parts of the Americas are particularly affected by floods. Earthquakes cause more severe economic impacts, particularly in regions like Japan and Chile, whereas floods result in widespread population damage but on a smaller financial scale. Countries such as Indonesia, India, Bangladesh, and Nepal are notably vulnerable to both disaster types, experiencing varying degrees of financial damage and loss of life.\nFlood-prone regions in Asia consistently show higher numbers of affected people, while earthquake-prone regions report a higher number of fatalities despite fewer people being affected. This analysis emphasizes the need for tailored disaster preparedness strategies in regions vulnerable to both floods and earthquakes. Additionally, the importance of resilient infrastructure in reducing the economic impact of such disasters is highlighted, especially in countries with large populations and high susceptibility to natural hazards.\nThis approach provides a clear understanding of where the most devastating disasters occur, helping to prioritize resources for more effective disaster management and mitigation efforts in the most impacted regions."
  },
  {
    "objectID": "code.html#research-question-3-relationship-between-disaster-magnitude-and-impact",
    "href": "code.html#research-question-3-relationship-between-disaster-magnitude-and-impact",
    "title": "Source Code for Research Questions",
    "section": "",
    "text": "df = read.csv(\"C:\\\\Users\\\\karti\\\\OneDrive\\\\Desktop\\\\DISASTERS_1988.csv\")\ncolnames(df)\n# Load the dplyr library\nlibrary(dplyr)\ndf &lt;- df %&gt;%\n  rename(Total_Damage_USD = Total_Damage_.US...)\n\nGeo &lt;- df %&gt;%\n  select(Disaster.Type, Country, Region, Magnitude, Magnitude.Scale, Location, Latitude, Longitude, Total.Affected, Total.Deaths, Total_Damage_USD)\n\nhead(Geo)\nsummary(Geo)\n\n# Count of missing values in each column\ncolSums(is.na(Geo))\n\n# Filter out rows with missing values in required columns\nGeo &lt;- Geo %&gt;%\n  filter(!is.na(Latitude) & !is.na(Longitude) & \n           !is.na(Total.Deaths) & !is.na(Total.Affected) & !is.na(Magnitude))\n\n\n\n\n\n# Check the frequency of occurrences per country\ncountry_distribution &lt;- Geo %&gt;%\n  count(Country, sort = TRUE)\n\n# View the top countries\nprint(country_distribution)\n\n# Summarize region-wise distribution\nregion_distribution &lt;- Geo %&gt;%\n  group_by(Region) %&gt;%\n  summarise(\n    Total_Affected = sum(Total.Affected, na.rm = TRUE),\n    Total_Deaths = sum(Total.Deaths, na.rm = TRUE),\n    Total_Damage = sum(Total_Damage_USD, na.rm = TRUE),\n    Count = n()\n  ) %&gt;%\n  arrange(desc(Total_Affected))  # Sort by Total Affected (or change sorting criteria)\n\n# View the summarized data\nprint(region_distribution)\n\n# Summarize frequency and totals per country\ncountry_distribution &lt;- Geo %&gt;%\n  group_by(Country) %&gt;%\n  summarise(\n    Frequency = n(),  # Number of disasters\n    Total_Deaths = sum(Total.Deaths, na.rm = TRUE),\n    Total_Affected = sum(Total.Affected, na.rm = TRUE),\n    Total_Damage = sum(Total_Damage_USD, na.rm = TRUE)\n  ) %&gt;%\n  arrange(desc(Frequency))  # Sort by frequency (or change to Total_Deaths/Total_Damage)\n\n# View the summarized data\nprint(country_distribution, n=150)\n\n\nlibrary(dplyr)\n\n# Calculate the disaster count for each country\nfiltered_geo &lt;- Geo %&gt;%\n  group_by(Country) %&gt;%\n  mutate(Disaster_Count = n()) %&gt;%  # Add a column with disaster count\n  ungroup() %&gt;% \n  filter(Disaster_Count &gt; 4) %&gt;%   # Keep only countries with disaster count &gt;= 4\n  select(-Disaster_Count)           # Optionally, remove the disaster count column\n\n# View the updated dataset\n\n# Summarize frequency and totals per country\ncountry_distribution2 &lt;- filtered_geo %&gt;%\n  group_by(Country) %&gt;%\n  summarise(\n    Frequency = n(),  # Number of disasters\n    Total_Deaths = sum(Total.Deaths, na.rm = TRUE),\n    Total_Affected = sum(Total.Affected, na.rm = TRUE),\n    Total_Damage = sum(Total_Damage_USD, na.rm = TRUE)\n  ) %&gt;%\n  arrange(desc(Frequency))  # Sort by frequency (or change to Total_Deaths/Total_Damage)\n\n# View the summarized data\nprint(country_distribution2, n=100)\n\n\n# Summarize region-wise distribution\nregion_distribution &lt;- filtered_geo %&gt;%\n  group_by(Region) %&gt;%\n  summarise(\n    Total_Affected = sum(Total.Affected, na.rm = TRUE),\n    Total_Deaths = sum(Total.Deaths, na.rm = TRUE),\n    Total_Damage = sum(Total_Damage_USD, na.rm = TRUE),\n    Count = n()\n  ) %&gt;%\n  arrange(desc(Total_Affected))  # Sort by Total Affected (or change sorting criteria)\n\n# View the summarized data\nprint(region_distribution)\n\n# Filter out Oceania from the dataset\nfiltered_geo &lt;- filtered_geo %&gt;%\n  filter(Region != \"Oceania\")\n\nsummary(filtered_geo)\n\nnumeric_data &lt;- filtered_geo %&gt;%\n  select(where(is.numeric))\n\n# Compute the correlation matrix\ncorrelation_matrix &lt;- cor(numeric_data, use = \"complete.obs\")\n\n# View the correlation matrix\nprint(correlation_matrix)\n\nsummary(filtered_geo$Total_Damage_USD)\n\n# Replace missing values with the median\nmedian_damage &lt;- median(filtered_geo$Total_Damage_USD, na.rm = TRUE)\nfiltered_geo &lt;- filtered_geo %&gt;%\n  mutate(\n    Total_Damage_USD = ifelse(is.na(Total_Damage_USD), median_damage, Total_Damage_USD)\n  )\nsummary(filtered_geo$Total_Damage_USD)\n\n# Ensure Disaster.Type column has no leading/trailing spaces and is in lowercase\nfiltered_geo$Disaster.Type &lt;- tolower(trimws(filtered_geo$Disaster.Type))\n\n# Filter out rows where Disaster.Type is \"storm\"\nfiltered_geo &lt;- filtered_geo[filtered_geo$Disaster.Type != \"storm\", ]\n\n# Check the distribution again\ndisaster_type_distribution &lt;- filtered_geo %&gt;%\n  count(Disaster.Type, sort = TRUE)\n\n# View the updated distribution\nprint(disaster_type_distribution)\n\n\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(plotly)\n\nvisualize_disaster_types_by_country &lt;- function() {\n  # Summarize the data by Disaster Type and Country\n  disaster_summary &lt;- filtered_geo %&gt;%\n    group_by(Country, Disaster.Type) %&gt;%\n    summarise(\n      Total_Affected = sum(Total.Affected, na.rm = TRUE),\n      Total_Damage_USD = sum(Total_Damage_USD, na.rm = TRUE)\n    ) %&gt;%\n    ungroup() %&gt;%\n    arrange(desc(Total_Affected)) %&gt;%\n    top_n(20, Total_Affected)  # Select top 20 countries by Total Affected\n  \n  # Create the plot\n  plot &lt;- ggplot(disaster_summary, aes(\n    x = reorder(Country, Total_Affected), \n    y = Total_Affected,\n    fill = Disaster.Type,\n    text = paste(\n      \"Country:\", Country,\n      \"&lt;br&gt;Disaster Type:\", Disaster.Type,\n      \"&lt;br&gt;Total Affected:\", Total_Affected,\n      \"&lt;br&gt;Total Damage (USD):\", Total_Damage_USD\n    )\n  )) +\n    geom_bar(stat = \"identity\", position = \"dodge\") +\n    scale_fill_brewer(palette = \"Set2\") +\n    labs(\n      title = \"Top 20 Countries Affected by Disaster Types\",\n      x = \"Country\",\n      y = \"Total Affected\",\n      fill = \"Disaster Type\"\n    ) +\n    theme_minimal() +\n    coord_flip()  # Flip coordinates for better readability\n  \n  # Render the plot with plotly for interactivity\n  ggplotly(plot, tooltip = \"text\")\n}\n\n# Call the function to create and display the visualization\nvisualize_disaster_types_by_country()\n\n\n# Filter data by regions\nasia_data &lt;- filtered_geo %&gt;% filter(Region == \"Asia\")\namericas_data &lt;- filtered_geo %&gt;% filter(Region == \"Americas\")\nafrica_data &lt;- filtered_geo %&gt;% filter(Region == \"Africa\")\neurope_data &lt;- filtered_geo %&gt;% filter(Region == \"Europe\")\n\n\n# Load libraries\nlibrary(dbscan)\nlibrary(sf)\nlibrary(ggplot2)\nlibrary(spatstat)\nlibrary(ggplot2)\nlibrary(plotly)\nlibrary(cluster)\nlibrary(tidyverse)\n\n\n\n\nperform_hdbscan_all_regions &lt;- function() {\n  # Load required libraries\n  if (!requireNamespace(\"dbscan\", quietly = TRUE)) install.packages(\"dbscan\")\n  if (!requireNamespace(\"plotly\", quietly = TRUE)) install.packages(\"plotly\")\n  if (!requireNamespace(\"maps\", quietly = TRUE)) install.packages(\"maps\")\n  \n  library(dbscan)\n  library(plotly)\n  library(maps)\n  library(dplyr)\n  library(ggplot2)\n  \n  # Load world map data\n  world_map &lt;- map_data(\"world\")\n  \n  # Filter and prepare the data for all regions\n  all_regions_data &lt;- filtered_geo %&gt;%\n    select(Region, Magnitude, Total.Affected, Total.Deaths, Total_Damage_USD, Longitude, Latitude, Disaster.Type) %&gt;%\n    drop_na()  # Remove rows with missing values\n  \n  # Scale the numerical data\n  scaled_data &lt;- scale(all_regions_data %&gt;% select(Magnitude, Total.Affected, Total.Deaths, Total_Damage_USD, Longitude, Latitude))\n  \n  # Perform HDBSCAN clustering\n  set.seed(42)  # Ensure reproducibility\n  hdbscan_result &lt;- hdbscan(scaled_data, minPts = 10)  # Adjust minPts as needed\n  \n  # Add cluster labels to the dataset\n  all_regions_data$Cluster &lt;- hdbscan_result$cluster\n  \n  # Remove noise points (Cluster = 0)\n  all_regions_data_filtered &lt;- all_regions_data %&gt;%\n    filter(Cluster != 0)\n  \n  # Create the base map with proper grouping\n  base_map &lt;- ggplot() +\n    geom_polygon(data = world_map, aes(x = long, y = lat, group = group), \n                 fill = \"gray90\", color = \"white\") +\n    coord_fixed(ratio = 1.3) +\n    theme_void()\n  \n  # Overlay clustering visualization\n  cluster_plot &lt;- base_map +\n    geom_point(data = all_regions_data_filtered, aes(x = Longitude, y = Latitude, \n                                                     color = as.factor(Cluster), \n                                                     shape = Disaster.Type, \n                                                     size = Total_Damage_USD / 1e6,\n                                                     text = paste(\n                                                       \"Region:\", Region,\n                                                       \"&lt;br&gt;Cluster:\", Cluster,\n                                                       \"&lt;br&gt;Disaster Type:\", Disaster.Type,\n                                                       \"&lt;br&gt;Magnitude:\", Magnitude,\n                                                       \"&lt;br&gt;Total Affected:\", Total.Affected,\n                                                       \"&lt;br&gt;Total Deaths:\", Total.Deaths,\n                                                       \"&lt;br&gt;Total Damage (USD):\", Total_Damage_USD\n                                                     )), alpha = 0.8) +\n    labs(\n      title = \"HDBSCAN Clustering Analysis with World Map: All Regions\",\n      x = \"Longitude\", y = \"Latitude\", \n      color = \"Cluster\", \n      shape = \"Disaster Type\",\n      size = \"Total Damage (USD, scaled)\"\n    ) +\n    theme_minimal()\n  \n  # Render the plot with plotly for interactivity\n  ggplotly(cluster_plot, tooltip = \"text\")\n}\n\n\n# Call the function to visualize all regions\nperform_hdbscan_all_regions()"
  }
]