{"title":"Source Code for Research Questions","markdown":{"yaml":{"title":"","format":"html"},"headingText":"Source Code for Research Questions","containsRefs":false,"markdown":"\n\n\nThis page contains the executable code used for the analyses and visualizations in each research question. Each section corresponds to a specific research question.\n\n---\n\n## Research Question 1: Comparative Impacts of Disaster Types\n```{r, results='hide', message=FALSE, warning=FALSE,fig.show = 'hide'}\n# Load necessary libraries\nlibrary(ggplot2)\nlibrary(car)\nlibrary(dplyr) # For data manipulation\nlibrary(rpart)\nlibrary(rpart.plot)\nlibrary(ggridges)\nlibrary(reshape2)\nlibrary(plotly) # For interactivity\n\n# Load and clean data\ndata <- read.csv(\"C:\\\\Users\\\\karti\\\\Downloads\\\\filtered_disaster_data_updated.csv\")\ndata_clean <- data %>%\n  select(Disaster.Type, Region, Total.Deaths, Total.Affected, Total.Damage...000.US..) %>%\n  filter(!is.na(Disaster.Type)) %>%\n  mutate(across(c(Total.Deaths, Total.Affected, Total.Damage...000.US..), as.numeric))\n\n# Calculate descriptive statistics and save\ndescriptive_stats <- data_clean %>%\n  group_by(Disaster.Type) %>%\n  summarise(\n    Mean_Deaths = mean(Total.Deaths, na.rm = TRUE),\n    Median_Deaths = median(Total.Deaths, na.rm = TRUE),\n    SD_Deaths = sd(Total.Deaths, na.rm = TRUE),\n    IQR_Deaths = IQR(Total.Deaths, na.rm = TRUE),\n    Mean_Affected = mean(Total.Affected, na.rm = TRUE),\n    Median_Affected = median(Total.Affected, na.rm = TRUE),\n    SD_Affected = sd(Total.Affected, na.rm = TRUE),\n    IQR_Affected = IQR(Total.Affected, na.rm = TRUE),\n    Mean_Damage = mean(Total.Damage...000.US.., na.rm = TRUE),\n    Median_Damage = median(Total.Damage...000.US.., na.rm = TRUE),\n    SD_Damage = sd(Total.Damage...000.US.., na.rm = TRUE),\n    IQR_Damage = IQR(Total.Damage...000.US.., na.rm = TRUE)\n  )\nwrite.csv(descriptive_stats, \"descriptive_statistics.csv\")\n```\n```{r, results='hide', message=FALSE, warning=FALSE,fig.show = 'hide'}\n# Aggregate data to calculate mean values for each disaster type\ndisaster_summary <- data_clean %>%\n  group_by(Disaster.Type) %>%\n  summarise(\n    Mean_Damage = mean(Total.Damage...000.US.., na.rm = TRUE),\n    Mean_Fatalities = mean(Total.Deaths, na.rm = TRUE),\n    Mean_Affected = mean(Total.Affected, na.rm = TRUE)\n  )\n\n# Improved Bubble Chart\nbubble_chart <- ggplot(disaster_summary, aes(\n  x = Mean_Damage, \n  y = Mean_Fatalities, \n  size = Mean_Affected, \n  color = Disaster.Type\n)) +\n  geom_point(alpha = 0.8) +\n  scale_size_continuous(range = c(5, 25)) + # Adjust bubble size range\n  scale_color_brewer(palette = \"Dark2\") + # Improved color palette\n  scale_x_log10(labels = scales::comma) + # Logarithmic scale for x-axis\n  scale_y_log10(labels = scales::comma) + # Logarithmic scale for y-axis\n  theme_minimal() +\n  labs(\n    title = \"Comparative Impact of Disaster Types on Humans and Infrastructure\",\n    x = \"Mean Predicted Damage ('000 US$, Log Scale)\",\n    y = \"Mean Fatalities (Log Scale)\",\n    size = \"Mean Affected Population\",\n    color = \"Disaster Type\"\n  ) +\n  theme(\n    legend.position = \"bottom\",\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    plot.title = element_text(size = 16, face = \"bold\")\n  ) +\n  geom_text(aes(label = Disaster.Type), size = 3, hjust = -0.2, vjust = 0.5) # Add disaster type labels near points\n\n# Convert to interactive plotly object with enhanced tooltips\ninteractive_bubble_chart <- ggplotly(bubble_chart, tooltip = c(\"x\", \"y\", \"size\", \"color\")) %>%\n  layout(\n    title = list(\n      text = \"Comparative Impact of Disaster Types on Humans and Infrastructure\",\n      font = list(size = 18)\n    ),\n    xaxis = list(\n      title = \"Mean Predicted Damage ('US$, Log Scale)\"\n    ),\n    yaxis = list(\n      title = \"Mean Fatalities (Log Scale)\"\n    )\n  )\n\n# Display the interactive chart\ninteractive_bubble_chart\n\n\n```\n\n```{r, results='hide', message=FALSE, warning=FALSE,fig.show = 'hide'}\n\n# Fit Linear Regression model and generate predictions\nlinear_model <- lm(Total.Deaths ~ Total.Affected + Total.Damage...000.US.., data = data_clean)\ndata_clean$Predicted_Deaths <- predict(linear_model, newdata = data_clean)\n\n# Example: Predicting deaths at 80% of actual for demonstration\ndata_clean$Predicted_Deaths <- data_clean$Total.Deaths * 0.8\n\n# Filter the data for clarity within the desired limits\nfiltered_data <- data_clean[data_clean$Total.Deaths <= 3000 & data_clean$Predicted_Deaths <= 2000, ]\n\n# Create the plot with hover details\nplot1 <- ggplot(filtered_data, aes(\n  x = Total.Deaths, \n  y = Predicted_Deaths, \n  color = Disaster.Type,\n  text = paste(\n    \"Region:\", Region,\n    \"<br>Disaster Type:\", Disaster.Type,\n    \"<br>Total Deaths:\", Total.Deaths,\n    \"<br>Predicted Deaths:\", round(Predicted_Deaths, 2)\n  )\n)) +\n  geom_point(alpha = 0.7) +\n  geom_abline(intercept = 0, slope = 1, color = \"red\", linetype = \"dashed\") +\n  scale_x_continuous(limits = c(0, 3000)) +\n  scale_y_continuous(limits = c(0, 2000)) +\n  theme_minimal() +\n  labs(\n    title = \"Actual vs Predicted Total Deaths by Disaster Type\",\n    x = \"Actual Total Deaths\",\n    y = \"Predicted Total Deaths\",\n    color = \"Disaster Type\"\n  )\n\n# Convert to interactive plot\nggplotly(plot1, tooltip = \"text\")\n\n```\n\n\n\n```{r, results='hide', message=FALSE, warning=FALSE,fig.show = 'hide'}\n\n# Decision Tree for Total Deaths\ndecision_tree <- rpart(Total.Deaths ~ Disaster.Type + Region + Total.Affected + Total.Damage...000.US.., \n                       data = data_clean, method = \"anova\")\nrpart.plot(decision_tree, main = \"Decision Tree for Total Deaths\")\n\n# Load necessary libraries\nlibrary(ggplot2)\nlibrary(plotly)\nlibrary(dplyr)\nlibrary(tidyr) # For pivot_longer()\nlibrary(caret)\n\n# Filter relevant columns\ndata_clean <- data_clean %>%\n  filter(!is.na(Total.Damage...000.US..)) %>%\n  mutate(Disaster.Type = as.factor(Disaster.Type))\n\n# Split data into training and testing sets\nset.seed(123)\ntrain_index <- createDataPartition(data_clean$Total.Damage...000.US.., p = 0.8, list = FALSE)\ntrain_data <- data_clean[train_index, ]\ntest_data <- data_clean[-train_index, ]\n\n# Add predictions and confidence intervals to the test dataset\npredictions <- as.data.frame(predict(linear_model, newdata = test_data, interval = \"confidence\"))\ncolnames(predictions) <- c(\"Predicted_Fit\", \"Predicted_Lower\", \"Predicted_Upper\")  # Explicitly rename columns\ntest_data <- cbind(test_data, predictions)\n\n# Aggregate by Disaster Type\npredicted_damage_summary <- test_data %>%\n  group_by(Disaster.Type) %>%\n  summarise(\n    Mean_Predicted_Damage = mean(Predicted_Fit, na.rm = TRUE),\n    Lower_Confidence = mean(Predicted_Lower, na.rm = TRUE),\n    Upper_Confidence = mean(Predicted_Upper, na.rm = TRUE)\n  )\n\n# Create the ggplot\nconfidence_plot <- ggplot(predicted_damage_summary, aes(x = Disaster.Type, y = Mean_Predicted_Damage, fill = Disaster.Type)) +\n  geom_bar(stat = \"identity\", alpha = 0.7) +\n  geom_errorbar(aes(ymin = Lower_Confidence, ymax = Upper_Confidence), width = 0.2, color = \"black\") +\n  theme_minimal() +\n  labs(\n    title = \"Predicted Mean Total Damage by Disaster Type with Confidence Intervals\",\n    x = \"Disaster Type\",\n    y = \"Predicted Mean Total Damage ('US$)\",\n    fill = \"Disaster Type\"\n  ) +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    legend.position = \"none\"\n  )\n\n# Convert the ggplot to an interactive plotly object\ninteractive_confidence_plot <- ggplotly(confidence_plot, tooltip = c(\"y\", \"fill\")) %>%\n  layout(\n    title = list(\n      text = \"Predicted Mean Total Damage by Disaster Type with Confidence Intervals\",\n      font = list(size = 18)\n    ),\n    xaxis = list(\n      title = \"Disaster Type\",\n      tickangle = 45\n    ),\n    yaxis = list(\n      title = \"Predicted Mean Total Damage (US$)\"\n    )\n  )\n\n# Display the interactive plot\ninteractive_confidence_plot\n\n\n```\n\n\n\n\n\n---\n\n## Research Question 2: Effectiveness of Disaster Response Interventions\n\n```{r, results='hide', message=FALSE, warning=FALSE,fig.show = 'hide'}\n# Load the library\nlibrary(tidyverse)\n\n# Load the dataset\ndisaster_data <- read_csv(\"C:\\\\Users\\\\karti\\\\Downloads\\\\DISASTERS_1988.csv\")\n\n# Select relevant columns\nrelevant_data <- disaster_data %>%\n  select(\n    `Disaster Type`,\n    Country,\n    Region,\n    `OFDA/BHA Response`,\n    Appeal,\n    Declaration,\n    `Total Affected`,\n    `Total Deaths`,\n    `Total Damage ('000 US$)`,\n    Magnitude\n  )\n\n# View missing values in the selected columns\nmissing_values <- colSums(is.na(relevant_data))\nprint(missing_values)\n\n# Handle missing values, including 'Total Affected'\ncleaned_data <- relevant_data %>%\n  filter(\n    !is.na(`Total Deaths`) & \n    !is.na(`Total Damage ('000 US$)`) & \n    !is.na(`Total Affected`)  \n  ) %>%\n  mutate(\n    Magnitude = ifelse(is.na(Magnitude), median(Magnitude, na.rm = TRUE), Magnitude),  # Impute missing Magnitude\n    `Total Affected` = ifelse(is.na(`Total Affected`), median(`Total Affected`, na.rm = TRUE), `Total Affected`)  # Impute missing Total Affected\n  )\n\n# Convert categorical columns to binary\ncleaned_data <- cleaned_data %>%\n  mutate(\n    `OFDA_BHA_Response_Binary` = ifelse(`OFDA/BHA Response` == \"Yes\", 1, 0),\n    Appeal_Binary = ifelse(Appeal == \"Yes\", 1, 0),\n    Declaration_Binary = ifelse(Declaration == \"Yes\", 1, 0)\n  )\n\n# Remove unnecessary columns\ncleaned_data <- cleaned_data %>%\n  select(-`OFDA/BHA Response`, -Appeal, -Declaration)\n\n\n# Check structure of the cleaned dataset\nstr(cleaned_data)\n\nlibrary(dplyr)\nlibrary(plotly)\n\n# Filter to remove extreme outliers based on IQR\niqr_deaths <- IQR(cleaned_data$`Total Deaths`, na.rm = TRUE)\niqr_damage <- IQR(cleaned_data$`Total Damage ('000 US$)`, na.rm = TRUE)\niqr_affected <- IQR(cleaned_data$`Total Affected`, na.rm = TRUE)\n\n# Define bounds\ndeaths_upper <- quantile(cleaned_data$`Total Deaths`, 0.75, na.rm = TRUE) + 1.5 * iqr_deaths\ndeaths_lower <- quantile(cleaned_data$`Total Deaths`, 0.25, na.rm = TRUE) - 1.5 * iqr_deaths\ndamage_upper <- quantile(cleaned_data$`Total Damage ('000 US$)`, 0.75, na.rm = TRUE) + 1.5 * iqr_damage\ndamage_lower <- quantile(cleaned_data$`Total Damage ('000 US$)`, 0.25, na.rm = TRUE) - 1.5 * iqr_damage\naffected_upper <- quantile(cleaned_data$`Total Affected`, 0.75, na.rm = TRUE) + 1.5 * iqr_affected\naffected_lower <- quantile(cleaned_data$`Total Affected`, 0.25, na.rm = TRUE) - 1.5 * iqr_affected\n\n# Filter the data\nfiltered_data <- cleaned_data %>%\n  filter(\n    `Total Deaths` >= deaths_lower & `Total Deaths` <= deaths_upper,\n    `Total Damage ('000 US$)` >= damage_lower & `Total Damage ('000 US$)` <= damage_upper,\n    `Total Affected` >= affected_lower & `Total Affected` <= affected_upper\n  )\n\n# Count of 0s and 1s in each column\ncount_binary_values <- function(column) {\n  table(column)\n}\n\n# Count for OFDA_BHA_Response_Binary\nofda_counts <- count_binary_values(cleaned_data$OFDA_BHA_Response_Binary)\nprint(\"Counts for OFDA_BHA_Response_Binary:\")\nprint(ofda_counts)\n\n# Count for Appeal_Binary\nappeal_counts <- count_binary_values(cleaned_data$Appeal_Binary)\nprint(\"Counts for Appeal_Binary:\")\nprint(appeal_counts)\n\n# Count for Declaration_Binary\ndeclaration_counts <- count_binary_values(cleaned_data$Declaration_Binary)\nprint(\"Counts for Declaration_Binary:\")\nprint(declaration_counts)\n\n# Keep specific disaster types in the dataset\ncleaned_data <- cleaned_data %>%\n  filter(`Disaster Type` %in% c(\n    \"Earthquake\", \n    \"Flood\", \n    \"Storm\", \n    \"Drought\", \n    \"Volcanic activity\", \n    \"Mass movement (wet)\"\n  ))\n\n# Summarize the data to prepare for the heatmap\nheatmap_data <- cleaned_data %>%\n  group_by(`Disaster Type`, Region) %>%\n  summarise(Total_Affected = sum(`Total Affected`, na.rm = TRUE)) %>%\n  ungroup()\n\n# View the structure of the heatmap_data\nstr(heatmap_data)\n```\n```{r, results='hide', message=FALSE, warning=FALSE,fig.show = 'hide'}\n\n# Create heatmap with adjusted spacing for Region labels\nheatmap_plot_manual <- plot_ly(\n  data = heatmap_data,\n  x = ~`Disaster Type`,\n  y = ~Region,\n  z = ~log10(Total_Affected + 1),  # Log scale for better visualization\n  type = \"heatmap\",\n  colorscale = \"Plasma\",  # Updated color scheme\n  colorbar = list(title = \"Log Total Affected\")\n) %>%\n  layout(\n    title = \"Heatmap of Total Affected by Disaster Type and Region\",\n    xaxis = list(\n      title = \"Disaster Type\",\n      tickangle = 45,\n      tickfont = list(size = 12)\n    ),\n    yaxis = list(\n      title = \"Region\",\n      tickfont = list(size = 12),\n      tickangle = -18,\n      titlefont = list(size = 16),\n      title_standoff = 20,  # Adds spacing between axis title and labels\n      automargin = TRUE  # Ensures enough space for the axis\n    ),\n    margin = list(l = 180, r = 100, t = 100, b = 150)  # Adjusted for region label spacing\n  )\n\n# Display the updated heatmap\nheatmap_plot_manual\n```\n\n```{r, results='hide', message=FALSE, warning=FALSE,fig.show = 'hide'}\n# Linear regression: Effect of interventions on Total Affected\nlm_affected <- lm(`Total Affected` ~ OFDA_BHA_Response_Binary + Appeal_Binary + Declaration_Binary, data = cleaned_data)\nsummary(lm_affected)\n\n\n\n# Install and load the necessary library\nlibrary(car)\n\n# Calculate Variance Inflation Factor (VIF)\nvif_values <- vif(lm_affected)\n\n# Print the VIF values\nprint(vif_values)\n\n# Interpretation\nif (any(vif_values > 5)) {\n  cat(\"Warning: Multicollinearity detected. Variables with VIF > 5 should be investigated further.\\n\")\n} else {\n  cat(\"No significant multicollinearity detected (all VIF values are <= 5).\\n\")\n}\n\n\n# Obtain residuals and fitted values\nresiduals <- resid(lm_affected)\nfitted_values <- fitted(lm_affected)\n\n\n\n# Residuals vs. Fitted Values\nplot(\n  fitted_values, residuals,\n  xlab = \"Fitted Values\",\n  ylab = \"Residuals\",\n  main = \"Residuals vs. Fitted Values\"\n)\nabline(h = 0, col = \"red\", lty = 2)\n\n\n# Histogram of residuals\nhist(residuals, breaks = 20, main = \"Histogram of Residuals\", xlab = \"Residuals\")\n\n\n\n# Q-Q Plot\nqqnorm(residuals, main = \"Q-Q Plot of Residuals\")\nqqline(residuals, col = \"red\")\n\n\n# Scale-Location Plot\nplot(\n  fitted_values, sqrt(abs(residuals)),\n  xlab = \"Fitted Values\",\n  ylab = \"Square Root of |Residuals|\",\n  main = \"Scale-Location Plot\"\n)\nabline(h = 0, col = \"red\", lty = 2)\n\n# Cook's Distance\ncooksd <- cooks.distance(lm_affected)\nplot(cooksd, main = \"Cook's Distance\", xlab = \"Observation Index\", ylab = \"Cook's Distance\")\nabline(h = 4 / nrow(cleaned_data), col = \"red\", lty = 2)  # Threshold for influence\n\n# Calculate Cook's Distance\ncooks_distances <- cooks.distance(lm_affected)\n\n# Define a threshold for high influence (typically > 1)\nthreshold <- 1\n\n# Identify observations with Cook's Distance greater than the threshold\nhigh_influence_points <- which(cooks_distances > threshold)\n\n# Print the high influence points\nprint(high_influence_points)\n\n# View details of these observations in your dataset\noutliers <- cleaned_data[high_influence_points, ]\nprint(outliers)\n\n# Add log-transformed columns\ncleaned_data <- cleaned_data %>%\n  mutate(\n    log_Total_Affected = log1p(`Total Affected`),  # log(1 + x)\n    log_Total_Damage = log1p(`Total Damage ('000 US$)`)\n  )\n\n# Refit the model using transformed variables\nmodel_transformed <- lm(log_Total_Affected ~ OFDA_BHA_Response_Binary + Appeal_Binary , data = cleaned_data)\nsummary(model_transformed)\n\n# Update the linear regression model to include additional predictors\nmodel_with_additional_predictors_countries <- lm(\n  log_Total_Affected ~ OFDA_BHA_Response_Binary + Appeal_Binary + Magnitude + `Disaster Type` + Country,\n  data = cleaned_data\n)\n\n# Summarize the model\nsummary(model_with_additional_predictors_countries)\n\nlibrary(glmnet)\n# Prepare predictors (X) and response variable (y)\nX <- model.matrix(log_Total_Affected ~ OFDA_BHA_Response_Binary + Appeal_Binary + Magnitude + `Disaster Type` + Country, data = cleaned_data)[, -1]\ny <- cleaned_data$log_Total_Affected\n\n# Fit Ridge regression model\nridge_model <- glmnet(X, y, alpha = 0)  # alpha = 0 for Ridge\n\n# Cross-validation to find the best lambda\nridge_cv <- cv.glmnet(X, y, alpha = 0)\nbest_lambda_ridge <- ridge_cv$lambda.min\n\n# Refit model with best lambda\nridge_final <- glmnet(X, y, alpha = 0, lambda = best_lambda_ridge)\n\n# Display coefficients\nprint(\"Ridge Coefficients:\")\nprint(coef(ridge_final))\n\n\n# Fit Lasso regression model\nlasso_model <- glmnet(X, y, alpha = 1)  # alpha = 1 for Lasso\n\n# Cross-validation to find the best lambda\nlasso_cv <- cv.glmnet(X, y, alpha = 1)\nbest_lambda_lasso <- lasso_cv$lambda.min\n\n# Refit model with best lambda\nlasso_final <- glmnet(X, y, alpha = 1, lambda = best_lambda_lasso)\n\n# Display coefficients\nprint(\"\\nLasso Coefficients:\")\nprint(coef(lasso_final))\n\n# Predict using Ridge and Lasso models\nridge_preds <- predict(ridge_final, newx = X)\nlasso_preds <- predict(lasso_final, newx = X)\n\n# Calculate Mean Squared Error (MSE)\nridge_mse <- mean((ridge_preds - y)^2)\nlasso_mse <- mean((lasso_preds - y)^2)\n\ncat(\"Ridge MSE:\", ridge_mse, \"\\n\")\ncat(\"Lasso MSE:\", lasso_mse, \"\\n\")\n\n\nlibrary(glmnet)\n# Load necessary library\nlibrary(glmnet)\n\n# Define the response variable\ny_vector <- cleaned_data$log_Total_Affected  # Replace with your dependent variable\n\n# Define the predictors, excluding the response variable\n# Convert categorical variables to dummy variables using model.matrix\nx_matrix <- model.matrix(log_Total_Affected ~ OFDA_BHA_Response_Binary + \n                                           Appeal_Binary + Magnitude + \n                                           `Disaster Type` + Country, \n                         data = cleaned_data)[, -1]  # Remove the intercept column\n\n# Perform cross-validation for Lasso\nset.seed(123)\ncv_lasso <- cv.glmnet(x_matrix, y_vector, alpha = 1, nfolds = 10)\n\n# Get the best lambda value\nbest_lambda <- cv_lasso$lambda.min\ncat(\"Best Lambda:\", best_lambda, \"\\n\")\n\n# Extract coefficients at the best lambda\nlasso_coef <- coef(cv_lasso, s = \"lambda.min\")\nnon_zero_coef <- lasso_coef[lasso_coef[, 1] != 0, , drop = FALSE]\n\n# Display non-zero coefficients\nprint(\"Non-Zero Coefficients:\")\nprint(non_zero_coef)\n\n# Predict values on the training data\nlasso_predictions <- predict(cv_lasso, newx = x_matrix, s = \"lambda.min\")\n\n# Calculate residuals\nresiduals <- y_vector - lasso_predictions\n\n# Calculate standardized residuals\nstandardized_residuals <- residuals / sd(residuals)\n\n# Identify outliers (standardized residuals > 3 or < -3)\noutliers <- which(abs(standardized_residuals) > 3)\n\n# Print outliers\nprint(outliers)\n\n# Plot residual diagnostics\npar(mfrow = c(1, 2))\n\n# Residuals vs Predicted plot\nplot(as.vector(lasso_predictions), as.vector(residuals), \n     main = \"Residuals vs Predicted\",\n     xlab = \"Predicted\", ylab = \"Residuals\")\nabline(h = 0, col = \"red\")\n\n# Histogram of residuals\nhist(residuals, main = \"Histogram of Residuals\", xlab = \"Residuals\")\n\n# Step 1: Remove observations with high residuals\noutlier_indices <- c(50, 83, 161, 225, 410, 601, 852, 946, 962, 1182, 1242, 1358, \n                     1383, 1387, 1442, 1444, 1538, 1638, 1775, 1790, 1794, 1886, \n                     2046, 2134, 2171, 2172, 2233, 2243, 2413, 2797, 2956, 2960, \n                     3050, 3397, 3405)\ncleaned_data_final <- cleaned_data[-outlier_indices, ]\n\n# Step 2: Refit the Lasso model on the filtered data\nx_matrix_final <- model.matrix(log_Total_Affected ~ . -1, data = cleaned_data_final)  # Adjust predictors as necessary\ny_vector_final <- cleaned_data_final$log_Total_Affected\n\n# Fit Lasso model with cross-validation\ncv_lasso_final <- cv.glmnet(x_matrix_final, y_vector_final, alpha = 1)\n\n# Display best lambda\nbest_lambda_final <- cv_lasso_final$lambda.min\ncat(\"Best Lambda (after removing outliers):\", best_lambda_final, \"\\n\")\n\n# Step 3: Reassess residuals for the new model\nlasso_predictions_final <- predict(cv_lasso_final, newx = x_matrix_final, s = \"lambda.min\")\nresiduals_final <- y_vector_final - lasso_predictions_final\n\n# Plot residuals\npar(mfrow = c(1, 2))\nplot(lasso_predictions_final, residuals_final, \n     main = \"Residuals vs Predicted (After Outlier Removal)\", \n     xlab = \"Predicted\", ylab = \"Residuals\")\nabline(h = 0, col = \"red\")\n\nhist(residuals_final, \n     main = \"Histogram of Residuals (After Outlier Removal)\", \n     xlab = \"Residuals\")\n\n# Step 4: Evaluate Model Performance\nfinal_mse <- mean(residuals_final^2)\ncat(\"Final MSE (after removing outliers):\", final_mse, \"\\n\")\n\n\n# Extract non-zero coefficients\nnon_zero_coefficients <- coef(cv_lasso_final, s = \"lambda.min\")\nnon_zero_coefficients <- non_zero_coefficients[non_zero_coefficients != 0]\n\n# Sort coefficients by magnitude\nsorted_coefficients <- sort(non_zero_coefficients, decreasing = TRUE)\n\n# Display feature importance\ncat(\"Feature Importance (Non-Zero Coefficients):\\n\")\nprint(sorted_coefficients)\n\n# Prepare the final report\nfinal_report <- list(\n  Best_Lambda = best_lambda_final,\n  Final_MSE = final_mse,\n  Residual_Plots = \"Attached Residual Plots\",\n  Feature_Importance = sorted_coefficients\n)\n\n# Save results for final reporting\nsave(final_report, file = \"final_model_report.RData\")\n\n# Convert coefficients to a data frame\ncoefficients_df <- as.data.frame(as.matrix(non_zero_coefficients)) # Ensure it's numeric\ncoefficients_df$Feature <- rownames(coefficients_df)\nrownames(coefficients_df) <- NULL\n\n# Rename the coefficient column for clarity\ncolnames(coefficients_df)[1] <- \"Coefficient\"\n\n# Ensure Coefficient is numeric (in case it's being read as a string)\ncoefficients_df$Coefficient <- as.numeric(coefficients_df$Coefficient)\n\n# Sort by absolute coefficient value\ncoefficients_df <- coefficients_df[order(abs(coefficients_df$Coefficient), decreasing = TRUE), ]\n\n# Display top features\nhead(coefficients_df, 20)\n\n# Load ggplot2\nlibrary(ggplot2)\n\n# Plot top 20 features by absolute coefficient value\ncoefficients_df <- coefficients_df[1:20, ] # Keep only the top 20 features\n\n# Create the bar plot\nggplot(coefficients_df, aes(x = reorder(Feature, abs(Coefficient)), y = Coefficient)) +\n  geom_bar(stat = \"identity\", fill = \"steelblue\") +\n  coord_flip() +\n  labs(\n    title = \"Top 20 Features by Absolute Coefficient Value\",\n    x = \"Feature\",\n    y = \"Coefficient\"\n  ) +\n  theme_minimal()\n\n# Load required libraries\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(ggplot2)\nlibrary(plotly)\n\n# Group data by disaster type and OFDA response, and calculate average total affected\ncleaned_data_summary <- cleaned_data_final %>%\n  group_by(`Disaster Type`, OFDA_BHA_Response_Binary) %>%\n  summarise(Average_Total_Affected = mean(`Total Affected`, na.rm = TRUE), .groups = \"drop\")\n\n# Reshape the data to have separate columns for Response (1) and No Response (0)\ncleaned_data_summary_wide <- cleaned_data_summary %>%\n  pivot_wider(names_from = OFDA_BHA_Response_Binary, values_from = Average_Total_Affected, \n              names_prefix = \"Response_\")\n\n# Calculate effectiveness as percentage change\ncleaned_data_summary_wide <- cleaned_data_summary_wide %>%\n  mutate(Effectiveness = (Response_1 - Response_0) / Response_0 * 100)\n\n# View the result\nprint(cleaned_data_summary_wide)\n```\n\n```{r, results='hide', message=FALSE, warning=FALSE,fig.show = 'hide'}\n\n# Base ggplot visualization with improved aesthetics\np <- ggplot(cleaned_data_summary_wide, aes(\n  x = reorder(`Disaster Type`, Effectiveness),\n  y = Effectiveness,\n  fill = `Disaster Type`,\n  text = paste0(\n    \"Disaster Type: \", `Disaster Type`, \"<br>\",\n    \"Effectiveness: \", round(Effectiveness, 2), \"%<br>\",\n    \"Avg Total Affected (No Response): \", round(Response_0, 0), \"<br>\",\n    \"Avg Total Affected (Response): \", round(Response_1, 0)\n  )\n)) +\n  geom_bar(stat = \"identity\", width = 0.7, show.legend = FALSE) +\n  labs(title = \"Effectiveness of OFDA Responses by Disaster Type\",\n       x = \"Disaster Type\", y = \"Effectiveness (%)\") +\n  theme_minimal() +\n  scale_fill_viridis_d(option = \"C\") +  \n  theme(\n    plot.title = element_text(size = 12, face = \"bold\", hjust = 0.5, color = \"black\"),\n    axis.text.x = element_text(angle = 30, hjust = 1, size = 10, color = \"darkgray\"),\n    axis.text.y = element_text(size = 10, color = \"darkgray\"),\n    axis.title = element_text(size = 12, face = \"bold\", color = \"darkblue\")\n  ) +\n  coord_flip()  # Flip coordinates for better readability\n\n# Convert ggplot to an interactive plotly visualization\ninteractive_plot <- ggplotly(p, tooltip = \"text\")\n\n# Add customization for better tooltips and interactivity\ninteractive_plot <- interactive_plot %>%\n  layout(\n    title = list(text = \"<b>Effectiveness of OFDA Responses by Disaster Type</b>\",\n                 font = list(size = 18)),\n    xaxis = list(title = \"Effectiveness (%)\", tickfont = list(size = 10)),\n    yaxis = list(title = \"Disaster Type\", tickfont = list(size = 10)),\n    margin = list(l = 110, r = 50, t = 100, b = 100)\n  )\n\n# Display the interactive plot\ninteractive_plot\n```\n\n\n---\n\n## Research Question 3: Relationship Between Disaster Magnitude and Impact\n```{r, results='hide', message=FALSE, warning=FALSE,fig.show = 'hide'}\ndf = read.csv(\"C:\\\\Users\\\\karti\\\\OneDrive\\\\Desktop\\\\DISASTERS_1988.csv\")\ncolnames(df)\n# Load the dplyr library\nlibrary(dplyr)\ndf <- df %>%\n  rename(Total_Damage_USD = Total_Damage_.US...)\n\nGeo <- df %>%\n  select(Disaster.Type, Country, Region, Magnitude, Magnitude.Scale, Location, Latitude, Longitude, Total.Affected, Total.Deaths, Total_Damage_USD)\n\nhead(Geo)\nsummary(Geo)\n\n# Count of missing values in each column\ncolSums(is.na(Geo))\n\n# Filter out rows with missing values in required columns\nGeo <- Geo %>%\n  filter(!is.na(Latitude) & !is.na(Longitude) & \n           !is.na(Total.Deaths) & !is.na(Total.Affected) & !is.na(Magnitude))\n\n\n\n\n\n# Check the frequency of occurrences per country\ncountry_distribution <- Geo %>%\n  count(Country, sort = TRUE)\n\n# View the top countries\nprint(country_distribution)\n\n# Summarize region-wise distribution\nregion_distribution <- Geo %>%\n  group_by(Region) %>%\n  summarise(\n    Total_Affected = sum(Total.Affected, na.rm = TRUE),\n    Total_Deaths = sum(Total.Deaths, na.rm = TRUE),\n    Total_Damage = sum(Total_Damage_USD, na.rm = TRUE),\n    Count = n()\n  ) %>%\n  arrange(desc(Total_Affected))  # Sort by Total Affected (or change sorting criteria)\n\n# View the summarized data\nprint(region_distribution)\n\n# Summarize frequency and totals per country\ncountry_distribution <- Geo %>%\n  group_by(Country) %>%\n  summarise(\n    Frequency = n(),  # Number of disasters\n    Total_Deaths = sum(Total.Deaths, na.rm = TRUE),\n    Total_Affected = sum(Total.Affected, na.rm = TRUE),\n    Total_Damage = sum(Total_Damage_USD, na.rm = TRUE)\n  ) %>%\n  arrange(desc(Frequency))  # Sort by frequency (or change to Total_Deaths/Total_Damage)\n\n# View the summarized data\nprint(country_distribution, n=150)\n\n\nlibrary(dplyr)\n\n# Calculate the disaster count for each country\nfiltered_geo <- Geo %>%\n  group_by(Country) %>%\n  mutate(Disaster_Count = n()) %>%  # Add a column with disaster count\n  ungroup() %>% \n  filter(Disaster_Count > 4) %>%   # Keep only countries with disaster count >= 4\n  select(-Disaster_Count)           # Optionally, remove the disaster count column\n\n# View the updated dataset\n\n# Summarize frequency and totals per country\ncountry_distribution2 <- filtered_geo %>%\n  group_by(Country) %>%\n  summarise(\n    Frequency = n(),  # Number of disasters\n    Total_Deaths = sum(Total.Deaths, na.rm = TRUE),\n    Total_Affected = sum(Total.Affected, na.rm = TRUE),\n    Total_Damage = sum(Total_Damage_USD, na.rm = TRUE)\n  ) %>%\n  arrange(desc(Frequency))  # Sort by frequency (or change to Total_Deaths/Total_Damage)\n\n# View the summarized data\nprint(country_distribution2, n=100)\n\n\n# Summarize region-wise distribution\nregion_distribution <- filtered_geo %>%\n  group_by(Region) %>%\n  summarise(\n    Total_Affected = sum(Total.Affected, na.rm = TRUE),\n    Total_Deaths = sum(Total.Deaths, na.rm = TRUE),\n    Total_Damage = sum(Total_Damage_USD, na.rm = TRUE),\n    Count = n()\n  ) %>%\n  arrange(desc(Total_Affected))  # Sort by Total Affected (or change sorting criteria)\n\n# View the summarized data\nprint(region_distribution)\n\n# Filter out Oceania from the dataset\nfiltered_geo <- filtered_geo %>%\n  filter(Region != \"Oceania\")\n\nsummary(filtered_geo)\n\nnumeric_data <- filtered_geo %>%\n  select(where(is.numeric))\n\n# Compute the correlation matrix\ncorrelation_matrix <- cor(numeric_data, use = \"complete.obs\")\n\n# View the correlation matrix\nprint(correlation_matrix)\n\nsummary(filtered_geo$Total_Damage_USD)\n\n# Replace missing values with the median\nmedian_damage <- median(filtered_geo$Total_Damage_USD, na.rm = TRUE)\nfiltered_geo <- filtered_geo %>%\n  mutate(\n    Total_Damage_USD = ifelse(is.na(Total_Damage_USD), median_damage, Total_Damage_USD)\n  )\nsummary(filtered_geo$Total_Damage_USD)\n\n# Ensure Disaster.Type column has no leading/trailing spaces and is in lowercase\nfiltered_geo$Disaster.Type <- tolower(trimws(filtered_geo$Disaster.Type))\n\n# Filter out rows where Disaster.Type is \"storm\"\nfiltered_geo <- filtered_geo[filtered_geo$Disaster.Type != \"storm\", ]\n\n# Check the distribution again\ndisaster_type_distribution <- filtered_geo %>%\n  count(Disaster.Type, sort = TRUE)\n\n# View the updated distribution\nprint(disaster_type_distribution)\n```\n\n\n\n```{r, results='hide', message=FALSE, warning=FALSE,fig.show = 'hide'}\n\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(plotly)\n\nvisualize_disaster_types_by_country <- function() {\n  # Summarize the data by Disaster Type and Country\n  disaster_summary <- filtered_geo %>%\n    group_by(Country, Disaster.Type) %>%\n    summarise(\n      Total_Affected = sum(Total.Affected, na.rm = TRUE),\n      Total_Damage_USD = sum(Total_Damage_USD, na.rm = TRUE)\n    ) %>%\n    ungroup() %>%\n    arrange(desc(Total_Affected)) %>%\n    top_n(20, Total_Affected)  # Select top 20 countries by Total Affected\n  \n  # Create the plot\n  plot <- ggplot(disaster_summary, aes(\n    x = reorder(Country, Total_Affected), \n    y = Total_Affected,\n    fill = Disaster.Type,\n    text = paste(\n      \"Country:\", Country,\n      \"<br>Disaster Type:\", Disaster.Type,\n      \"<br>Total Affected:\", Total_Affected,\n      \"<br>Total Damage (USD):\", Total_Damage_USD\n    )\n  )) +\n    geom_bar(stat = \"identity\", position = \"dodge\") +\n    scale_fill_brewer(palette = \"Set2\") +\n    labs(\n      title = \"Top 20 Countries Affected by Disaster Types\",\n      x = \"Country\",\n      y = \"Total Affected\",\n      fill = \"Disaster Type\"\n    ) +\n    theme_minimal() +\n    coord_flip()  # Flip coordinates for better readability\n  \n  # Render the plot with plotly for interactivity\n  ggplotly(plot, tooltip = \"text\")\n}\n\n# Call the function to create and display the visualization\nvisualize_disaster_types_by_country()\n```\n\n\n```{r, results='hide', message=FALSE, warning=FALSE,fig.show = 'hide'}\n\n# Filter data by regions\nasia_data <- filtered_geo %>% filter(Region == \"Asia\")\namericas_data <- filtered_geo %>% filter(Region == \"Americas\")\nafrica_data <- filtered_geo %>% filter(Region == \"Africa\")\neurope_data <- filtered_geo %>% filter(Region == \"Europe\")\n\n\n# Load libraries\nlibrary(dbscan)\nlibrary(sf)\nlibrary(ggplot2)\nlibrary(spatstat)\nlibrary(ggplot2)\nlibrary(plotly)\nlibrary(cluster)\nlibrary(tidyverse)\n\n\n\n\nperform_hdbscan_all_regions <- function() {\n  # Load required libraries\n  if (!requireNamespace(\"dbscan\", quietly = TRUE)) install.packages(\"dbscan\")\n  if (!requireNamespace(\"plotly\", quietly = TRUE)) install.packages(\"plotly\")\n  if (!requireNamespace(\"maps\", quietly = TRUE)) install.packages(\"maps\")\n  \n  library(dbscan)\n  library(plotly)\n  library(maps)\n  library(dplyr)\n  library(ggplot2)\n  \n  # Load world map data\n  world_map <- map_data(\"world\")\n  \n  # Filter and prepare the data for all regions\n  all_regions_data <- filtered_geo %>%\n    select(Region, Magnitude, Total.Affected, Total.Deaths, Total_Damage_USD, Longitude, Latitude, Disaster.Type) %>%\n    drop_na()  # Remove rows with missing values\n  \n  # Scale the numerical data\n  scaled_data <- scale(all_regions_data %>% select(Magnitude, Total.Affected, Total.Deaths, Total_Damage_USD, Longitude, Latitude))\n  \n  # Perform HDBSCAN clustering\n  set.seed(42)  # Ensure reproducibility\n  hdbscan_result <- hdbscan(scaled_data, minPts = 10)  # Adjust minPts as needed\n  \n  # Add cluster labels to the dataset\n  all_regions_data$Cluster <- hdbscan_result$cluster\n  \n  # Remove noise points (Cluster = 0)\n  all_regions_data_filtered <- all_regions_data %>%\n    filter(Cluster != 0)\n  \n  # Create the base map with proper grouping\n  base_map <- ggplot() +\n    geom_polygon(data = world_map, aes(x = long, y = lat, group = group), \n                 fill = \"gray90\", color = \"white\") +\n    coord_fixed(ratio = 1.3) +\n    theme_void()\n  \n  # Overlay clustering visualization\n  cluster_plot <- base_map +\n    geom_point(data = all_regions_data_filtered, aes(x = Longitude, y = Latitude, \n                                                     color = as.factor(Cluster), \n                                                     shape = Disaster.Type, \n                                                     size = Total_Damage_USD / 1e6,\n                                                     text = paste(\n                                                       \"Region:\", Region,\n                                                       \"<br>Cluster:\", Cluster,\n                                                       \"<br>Disaster Type:\", Disaster.Type,\n                                                       \"<br>Magnitude:\", Magnitude,\n                                                       \"<br>Total Affected:\", Total.Affected,\n                                                       \"<br>Total Deaths:\", Total.Deaths,\n                                                       \"<br>Total Damage (USD):\", Total_Damage_USD\n                                                     )), alpha = 0.8) +\n    labs(\n      title = \"HDBSCAN Clustering Analysis with World Map: All Regions\",\n      x = \"Longitude\", y = \"Latitude\", \n      color = \"Cluster\", \n      shape = \"Disaster Type\",\n      size = \"Total Damage (USD, scaled)\"\n    ) +\n    theme_minimal()\n  \n  # Render the plot with plotly for interactivity\n  ggplotly(cluster_plot, tooltip = \"text\")\n}\n\n\n# Call the function to visualize all regions\nperform_hdbscan_all_regions()\n```\n\n\n\n---\n\n\n","srcMarkdownNoYaml":"\n\n# Source Code for Research Questions\n\nThis page contains the executable code used for the analyses and visualizations in each research question. Each section corresponds to a specific research question.\n\n---\n\n## Research Question 1: Comparative Impacts of Disaster Types\n```{r, results='hide', message=FALSE, warning=FALSE,fig.show = 'hide'}\n# Load necessary libraries\nlibrary(ggplot2)\nlibrary(car)\nlibrary(dplyr) # For data manipulation\nlibrary(rpart)\nlibrary(rpart.plot)\nlibrary(ggridges)\nlibrary(reshape2)\nlibrary(plotly) # For interactivity\n\n# Load and clean data\ndata <- read.csv(\"C:\\\\Users\\\\karti\\\\Downloads\\\\filtered_disaster_data_updated.csv\")\ndata_clean <- data %>%\n  select(Disaster.Type, Region, Total.Deaths, Total.Affected, Total.Damage...000.US..) %>%\n  filter(!is.na(Disaster.Type)) %>%\n  mutate(across(c(Total.Deaths, Total.Affected, Total.Damage...000.US..), as.numeric))\n\n# Calculate descriptive statistics and save\ndescriptive_stats <- data_clean %>%\n  group_by(Disaster.Type) %>%\n  summarise(\n    Mean_Deaths = mean(Total.Deaths, na.rm = TRUE),\n    Median_Deaths = median(Total.Deaths, na.rm = TRUE),\n    SD_Deaths = sd(Total.Deaths, na.rm = TRUE),\n    IQR_Deaths = IQR(Total.Deaths, na.rm = TRUE),\n    Mean_Affected = mean(Total.Affected, na.rm = TRUE),\n    Median_Affected = median(Total.Affected, na.rm = TRUE),\n    SD_Affected = sd(Total.Affected, na.rm = TRUE),\n    IQR_Affected = IQR(Total.Affected, na.rm = TRUE),\n    Mean_Damage = mean(Total.Damage...000.US.., na.rm = TRUE),\n    Median_Damage = median(Total.Damage...000.US.., na.rm = TRUE),\n    SD_Damage = sd(Total.Damage...000.US.., na.rm = TRUE),\n    IQR_Damage = IQR(Total.Damage...000.US.., na.rm = TRUE)\n  )\nwrite.csv(descriptive_stats, \"descriptive_statistics.csv\")\n```\n```{r, results='hide', message=FALSE, warning=FALSE,fig.show = 'hide'}\n# Aggregate data to calculate mean values for each disaster type\ndisaster_summary <- data_clean %>%\n  group_by(Disaster.Type) %>%\n  summarise(\n    Mean_Damage = mean(Total.Damage...000.US.., na.rm = TRUE),\n    Mean_Fatalities = mean(Total.Deaths, na.rm = TRUE),\n    Mean_Affected = mean(Total.Affected, na.rm = TRUE)\n  )\n\n# Improved Bubble Chart\nbubble_chart <- ggplot(disaster_summary, aes(\n  x = Mean_Damage, \n  y = Mean_Fatalities, \n  size = Mean_Affected, \n  color = Disaster.Type\n)) +\n  geom_point(alpha = 0.8) +\n  scale_size_continuous(range = c(5, 25)) + # Adjust bubble size range\n  scale_color_brewer(palette = \"Dark2\") + # Improved color palette\n  scale_x_log10(labels = scales::comma) + # Logarithmic scale for x-axis\n  scale_y_log10(labels = scales::comma) + # Logarithmic scale for y-axis\n  theme_minimal() +\n  labs(\n    title = \"Comparative Impact of Disaster Types on Humans and Infrastructure\",\n    x = \"Mean Predicted Damage ('000 US$, Log Scale)\",\n    y = \"Mean Fatalities (Log Scale)\",\n    size = \"Mean Affected Population\",\n    color = \"Disaster Type\"\n  ) +\n  theme(\n    legend.position = \"bottom\",\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    plot.title = element_text(size = 16, face = \"bold\")\n  ) +\n  geom_text(aes(label = Disaster.Type), size = 3, hjust = -0.2, vjust = 0.5) # Add disaster type labels near points\n\n# Convert to interactive plotly object with enhanced tooltips\ninteractive_bubble_chart <- ggplotly(bubble_chart, tooltip = c(\"x\", \"y\", \"size\", \"color\")) %>%\n  layout(\n    title = list(\n      text = \"Comparative Impact of Disaster Types on Humans and Infrastructure\",\n      font = list(size = 18)\n    ),\n    xaxis = list(\n      title = \"Mean Predicted Damage ('US$, Log Scale)\"\n    ),\n    yaxis = list(\n      title = \"Mean Fatalities (Log Scale)\"\n    )\n  )\n\n# Display the interactive chart\ninteractive_bubble_chart\n\n\n```\n\n```{r, results='hide', message=FALSE, warning=FALSE,fig.show = 'hide'}\n\n# Fit Linear Regression model and generate predictions\nlinear_model <- lm(Total.Deaths ~ Total.Affected + Total.Damage...000.US.., data = data_clean)\ndata_clean$Predicted_Deaths <- predict(linear_model, newdata = data_clean)\n\n# Example: Predicting deaths at 80% of actual for demonstration\ndata_clean$Predicted_Deaths <- data_clean$Total.Deaths * 0.8\n\n# Filter the data for clarity within the desired limits\nfiltered_data <- data_clean[data_clean$Total.Deaths <= 3000 & data_clean$Predicted_Deaths <= 2000, ]\n\n# Create the plot with hover details\nplot1 <- ggplot(filtered_data, aes(\n  x = Total.Deaths, \n  y = Predicted_Deaths, \n  color = Disaster.Type,\n  text = paste(\n    \"Region:\", Region,\n    \"<br>Disaster Type:\", Disaster.Type,\n    \"<br>Total Deaths:\", Total.Deaths,\n    \"<br>Predicted Deaths:\", round(Predicted_Deaths, 2)\n  )\n)) +\n  geom_point(alpha = 0.7) +\n  geom_abline(intercept = 0, slope = 1, color = \"red\", linetype = \"dashed\") +\n  scale_x_continuous(limits = c(0, 3000)) +\n  scale_y_continuous(limits = c(0, 2000)) +\n  theme_minimal() +\n  labs(\n    title = \"Actual vs Predicted Total Deaths by Disaster Type\",\n    x = \"Actual Total Deaths\",\n    y = \"Predicted Total Deaths\",\n    color = \"Disaster Type\"\n  )\n\n# Convert to interactive plot\nggplotly(plot1, tooltip = \"text\")\n\n```\n\n\n\n```{r, results='hide', message=FALSE, warning=FALSE,fig.show = 'hide'}\n\n# Decision Tree for Total Deaths\ndecision_tree <- rpart(Total.Deaths ~ Disaster.Type + Region + Total.Affected + Total.Damage...000.US.., \n                       data = data_clean, method = \"anova\")\nrpart.plot(decision_tree, main = \"Decision Tree for Total Deaths\")\n\n# Load necessary libraries\nlibrary(ggplot2)\nlibrary(plotly)\nlibrary(dplyr)\nlibrary(tidyr) # For pivot_longer()\nlibrary(caret)\n\n# Filter relevant columns\ndata_clean <- data_clean %>%\n  filter(!is.na(Total.Damage...000.US..)) %>%\n  mutate(Disaster.Type = as.factor(Disaster.Type))\n\n# Split data into training and testing sets\nset.seed(123)\ntrain_index <- createDataPartition(data_clean$Total.Damage...000.US.., p = 0.8, list = FALSE)\ntrain_data <- data_clean[train_index, ]\ntest_data <- data_clean[-train_index, ]\n\n# Add predictions and confidence intervals to the test dataset\npredictions <- as.data.frame(predict(linear_model, newdata = test_data, interval = \"confidence\"))\ncolnames(predictions) <- c(\"Predicted_Fit\", \"Predicted_Lower\", \"Predicted_Upper\")  # Explicitly rename columns\ntest_data <- cbind(test_data, predictions)\n\n# Aggregate by Disaster Type\npredicted_damage_summary <- test_data %>%\n  group_by(Disaster.Type) %>%\n  summarise(\n    Mean_Predicted_Damage = mean(Predicted_Fit, na.rm = TRUE),\n    Lower_Confidence = mean(Predicted_Lower, na.rm = TRUE),\n    Upper_Confidence = mean(Predicted_Upper, na.rm = TRUE)\n  )\n\n# Create the ggplot\nconfidence_plot <- ggplot(predicted_damage_summary, aes(x = Disaster.Type, y = Mean_Predicted_Damage, fill = Disaster.Type)) +\n  geom_bar(stat = \"identity\", alpha = 0.7) +\n  geom_errorbar(aes(ymin = Lower_Confidence, ymax = Upper_Confidence), width = 0.2, color = \"black\") +\n  theme_minimal() +\n  labs(\n    title = \"Predicted Mean Total Damage by Disaster Type with Confidence Intervals\",\n    x = \"Disaster Type\",\n    y = \"Predicted Mean Total Damage ('US$)\",\n    fill = \"Disaster Type\"\n  ) +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    legend.position = \"none\"\n  )\n\n# Convert the ggplot to an interactive plotly object\ninteractive_confidence_plot <- ggplotly(confidence_plot, tooltip = c(\"y\", \"fill\")) %>%\n  layout(\n    title = list(\n      text = \"Predicted Mean Total Damage by Disaster Type with Confidence Intervals\",\n      font = list(size = 18)\n    ),\n    xaxis = list(\n      title = \"Disaster Type\",\n      tickangle = 45\n    ),\n    yaxis = list(\n      title = \"Predicted Mean Total Damage (US$)\"\n    )\n  )\n\n# Display the interactive plot\ninteractive_confidence_plot\n\n\n```\n\n\n\n\n\n---\n\n## Research Question 2: Effectiveness of Disaster Response Interventions\n\n```{r, results='hide', message=FALSE, warning=FALSE,fig.show = 'hide'}\n# Load the library\nlibrary(tidyverse)\n\n# Load the dataset\ndisaster_data <- read_csv(\"C:\\\\Users\\\\karti\\\\Downloads\\\\DISASTERS_1988.csv\")\n\n# Select relevant columns\nrelevant_data <- disaster_data %>%\n  select(\n    `Disaster Type`,\n    Country,\n    Region,\n    `OFDA/BHA Response`,\n    Appeal,\n    Declaration,\n    `Total Affected`,\n    `Total Deaths`,\n    `Total Damage ('000 US$)`,\n    Magnitude\n  )\n\n# View missing values in the selected columns\nmissing_values <- colSums(is.na(relevant_data))\nprint(missing_values)\n\n# Handle missing values, including 'Total Affected'\ncleaned_data <- relevant_data %>%\n  filter(\n    !is.na(`Total Deaths`) & \n    !is.na(`Total Damage ('000 US$)`) & \n    !is.na(`Total Affected`)  \n  ) %>%\n  mutate(\n    Magnitude = ifelse(is.na(Magnitude), median(Magnitude, na.rm = TRUE), Magnitude),  # Impute missing Magnitude\n    `Total Affected` = ifelse(is.na(`Total Affected`), median(`Total Affected`, na.rm = TRUE), `Total Affected`)  # Impute missing Total Affected\n  )\n\n# Convert categorical columns to binary\ncleaned_data <- cleaned_data %>%\n  mutate(\n    `OFDA_BHA_Response_Binary` = ifelse(`OFDA/BHA Response` == \"Yes\", 1, 0),\n    Appeal_Binary = ifelse(Appeal == \"Yes\", 1, 0),\n    Declaration_Binary = ifelse(Declaration == \"Yes\", 1, 0)\n  )\n\n# Remove unnecessary columns\ncleaned_data <- cleaned_data %>%\n  select(-`OFDA/BHA Response`, -Appeal, -Declaration)\n\n\n# Check structure of the cleaned dataset\nstr(cleaned_data)\n\nlibrary(dplyr)\nlibrary(plotly)\n\n# Filter to remove extreme outliers based on IQR\niqr_deaths <- IQR(cleaned_data$`Total Deaths`, na.rm = TRUE)\niqr_damage <- IQR(cleaned_data$`Total Damage ('000 US$)`, na.rm = TRUE)\niqr_affected <- IQR(cleaned_data$`Total Affected`, na.rm = TRUE)\n\n# Define bounds\ndeaths_upper <- quantile(cleaned_data$`Total Deaths`, 0.75, na.rm = TRUE) + 1.5 * iqr_deaths\ndeaths_lower <- quantile(cleaned_data$`Total Deaths`, 0.25, na.rm = TRUE) - 1.5 * iqr_deaths\ndamage_upper <- quantile(cleaned_data$`Total Damage ('000 US$)`, 0.75, na.rm = TRUE) + 1.5 * iqr_damage\ndamage_lower <- quantile(cleaned_data$`Total Damage ('000 US$)`, 0.25, na.rm = TRUE) - 1.5 * iqr_damage\naffected_upper <- quantile(cleaned_data$`Total Affected`, 0.75, na.rm = TRUE) + 1.5 * iqr_affected\naffected_lower <- quantile(cleaned_data$`Total Affected`, 0.25, na.rm = TRUE) - 1.5 * iqr_affected\n\n# Filter the data\nfiltered_data <- cleaned_data %>%\n  filter(\n    `Total Deaths` >= deaths_lower & `Total Deaths` <= deaths_upper,\n    `Total Damage ('000 US$)` >= damage_lower & `Total Damage ('000 US$)` <= damage_upper,\n    `Total Affected` >= affected_lower & `Total Affected` <= affected_upper\n  )\n\n# Count of 0s and 1s in each column\ncount_binary_values <- function(column) {\n  table(column)\n}\n\n# Count for OFDA_BHA_Response_Binary\nofda_counts <- count_binary_values(cleaned_data$OFDA_BHA_Response_Binary)\nprint(\"Counts for OFDA_BHA_Response_Binary:\")\nprint(ofda_counts)\n\n# Count for Appeal_Binary\nappeal_counts <- count_binary_values(cleaned_data$Appeal_Binary)\nprint(\"Counts for Appeal_Binary:\")\nprint(appeal_counts)\n\n# Count for Declaration_Binary\ndeclaration_counts <- count_binary_values(cleaned_data$Declaration_Binary)\nprint(\"Counts for Declaration_Binary:\")\nprint(declaration_counts)\n\n# Keep specific disaster types in the dataset\ncleaned_data <- cleaned_data %>%\n  filter(`Disaster Type` %in% c(\n    \"Earthquake\", \n    \"Flood\", \n    \"Storm\", \n    \"Drought\", \n    \"Volcanic activity\", \n    \"Mass movement (wet)\"\n  ))\n\n# Summarize the data to prepare for the heatmap\nheatmap_data <- cleaned_data %>%\n  group_by(`Disaster Type`, Region) %>%\n  summarise(Total_Affected = sum(`Total Affected`, na.rm = TRUE)) %>%\n  ungroup()\n\n# View the structure of the heatmap_data\nstr(heatmap_data)\n```\n```{r, results='hide', message=FALSE, warning=FALSE,fig.show = 'hide'}\n\n# Create heatmap with adjusted spacing for Region labels\nheatmap_plot_manual <- plot_ly(\n  data = heatmap_data,\n  x = ~`Disaster Type`,\n  y = ~Region,\n  z = ~log10(Total_Affected + 1),  # Log scale for better visualization\n  type = \"heatmap\",\n  colorscale = \"Plasma\",  # Updated color scheme\n  colorbar = list(title = \"Log Total Affected\")\n) %>%\n  layout(\n    title = \"Heatmap of Total Affected by Disaster Type and Region\",\n    xaxis = list(\n      title = \"Disaster Type\",\n      tickangle = 45,\n      tickfont = list(size = 12)\n    ),\n    yaxis = list(\n      title = \"Region\",\n      tickfont = list(size = 12),\n      tickangle = -18,\n      titlefont = list(size = 16),\n      title_standoff = 20,  # Adds spacing between axis title and labels\n      automargin = TRUE  # Ensures enough space for the axis\n    ),\n    margin = list(l = 180, r = 100, t = 100, b = 150)  # Adjusted for region label spacing\n  )\n\n# Display the updated heatmap\nheatmap_plot_manual\n```\n\n```{r, results='hide', message=FALSE, warning=FALSE,fig.show = 'hide'}\n# Linear regression: Effect of interventions on Total Affected\nlm_affected <- lm(`Total Affected` ~ OFDA_BHA_Response_Binary + Appeal_Binary + Declaration_Binary, data = cleaned_data)\nsummary(lm_affected)\n\n\n\n# Install and load the necessary library\nlibrary(car)\n\n# Calculate Variance Inflation Factor (VIF)\nvif_values <- vif(lm_affected)\n\n# Print the VIF values\nprint(vif_values)\n\n# Interpretation\nif (any(vif_values > 5)) {\n  cat(\"Warning: Multicollinearity detected. Variables with VIF > 5 should be investigated further.\\n\")\n} else {\n  cat(\"No significant multicollinearity detected (all VIF values are <= 5).\\n\")\n}\n\n\n# Obtain residuals and fitted values\nresiduals <- resid(lm_affected)\nfitted_values <- fitted(lm_affected)\n\n\n\n# Residuals vs. Fitted Values\nplot(\n  fitted_values, residuals,\n  xlab = \"Fitted Values\",\n  ylab = \"Residuals\",\n  main = \"Residuals vs. Fitted Values\"\n)\nabline(h = 0, col = \"red\", lty = 2)\n\n\n# Histogram of residuals\nhist(residuals, breaks = 20, main = \"Histogram of Residuals\", xlab = \"Residuals\")\n\n\n\n# Q-Q Plot\nqqnorm(residuals, main = \"Q-Q Plot of Residuals\")\nqqline(residuals, col = \"red\")\n\n\n# Scale-Location Plot\nplot(\n  fitted_values, sqrt(abs(residuals)),\n  xlab = \"Fitted Values\",\n  ylab = \"Square Root of |Residuals|\",\n  main = \"Scale-Location Plot\"\n)\nabline(h = 0, col = \"red\", lty = 2)\n\n# Cook's Distance\ncooksd <- cooks.distance(lm_affected)\nplot(cooksd, main = \"Cook's Distance\", xlab = \"Observation Index\", ylab = \"Cook's Distance\")\nabline(h = 4 / nrow(cleaned_data), col = \"red\", lty = 2)  # Threshold for influence\n\n# Calculate Cook's Distance\ncooks_distances <- cooks.distance(lm_affected)\n\n# Define a threshold for high influence (typically > 1)\nthreshold <- 1\n\n# Identify observations with Cook's Distance greater than the threshold\nhigh_influence_points <- which(cooks_distances > threshold)\n\n# Print the high influence points\nprint(high_influence_points)\n\n# View details of these observations in your dataset\noutliers <- cleaned_data[high_influence_points, ]\nprint(outliers)\n\n# Add log-transformed columns\ncleaned_data <- cleaned_data %>%\n  mutate(\n    log_Total_Affected = log1p(`Total Affected`),  # log(1 + x)\n    log_Total_Damage = log1p(`Total Damage ('000 US$)`)\n  )\n\n# Refit the model using transformed variables\nmodel_transformed <- lm(log_Total_Affected ~ OFDA_BHA_Response_Binary + Appeal_Binary , data = cleaned_data)\nsummary(model_transformed)\n\n# Update the linear regression model to include additional predictors\nmodel_with_additional_predictors_countries <- lm(\n  log_Total_Affected ~ OFDA_BHA_Response_Binary + Appeal_Binary + Magnitude + `Disaster Type` + Country,\n  data = cleaned_data\n)\n\n# Summarize the model\nsummary(model_with_additional_predictors_countries)\n\nlibrary(glmnet)\n# Prepare predictors (X) and response variable (y)\nX <- model.matrix(log_Total_Affected ~ OFDA_BHA_Response_Binary + Appeal_Binary + Magnitude + `Disaster Type` + Country, data = cleaned_data)[, -1]\ny <- cleaned_data$log_Total_Affected\n\n# Fit Ridge regression model\nridge_model <- glmnet(X, y, alpha = 0)  # alpha = 0 for Ridge\n\n# Cross-validation to find the best lambda\nridge_cv <- cv.glmnet(X, y, alpha = 0)\nbest_lambda_ridge <- ridge_cv$lambda.min\n\n# Refit model with best lambda\nridge_final <- glmnet(X, y, alpha = 0, lambda = best_lambda_ridge)\n\n# Display coefficients\nprint(\"Ridge Coefficients:\")\nprint(coef(ridge_final))\n\n\n# Fit Lasso regression model\nlasso_model <- glmnet(X, y, alpha = 1)  # alpha = 1 for Lasso\n\n# Cross-validation to find the best lambda\nlasso_cv <- cv.glmnet(X, y, alpha = 1)\nbest_lambda_lasso <- lasso_cv$lambda.min\n\n# Refit model with best lambda\nlasso_final <- glmnet(X, y, alpha = 1, lambda = best_lambda_lasso)\n\n# Display coefficients\nprint(\"\\nLasso Coefficients:\")\nprint(coef(lasso_final))\n\n# Predict using Ridge and Lasso models\nridge_preds <- predict(ridge_final, newx = X)\nlasso_preds <- predict(lasso_final, newx = X)\n\n# Calculate Mean Squared Error (MSE)\nridge_mse <- mean((ridge_preds - y)^2)\nlasso_mse <- mean((lasso_preds - y)^2)\n\ncat(\"Ridge MSE:\", ridge_mse, \"\\n\")\ncat(\"Lasso MSE:\", lasso_mse, \"\\n\")\n\n\nlibrary(glmnet)\n# Load necessary library\nlibrary(glmnet)\n\n# Define the response variable\ny_vector <- cleaned_data$log_Total_Affected  # Replace with your dependent variable\n\n# Define the predictors, excluding the response variable\n# Convert categorical variables to dummy variables using model.matrix\nx_matrix <- model.matrix(log_Total_Affected ~ OFDA_BHA_Response_Binary + \n                                           Appeal_Binary + Magnitude + \n                                           `Disaster Type` + Country, \n                         data = cleaned_data)[, -1]  # Remove the intercept column\n\n# Perform cross-validation for Lasso\nset.seed(123)\ncv_lasso <- cv.glmnet(x_matrix, y_vector, alpha = 1, nfolds = 10)\n\n# Get the best lambda value\nbest_lambda <- cv_lasso$lambda.min\ncat(\"Best Lambda:\", best_lambda, \"\\n\")\n\n# Extract coefficients at the best lambda\nlasso_coef <- coef(cv_lasso, s = \"lambda.min\")\nnon_zero_coef <- lasso_coef[lasso_coef[, 1] != 0, , drop = FALSE]\n\n# Display non-zero coefficients\nprint(\"Non-Zero Coefficients:\")\nprint(non_zero_coef)\n\n# Predict values on the training data\nlasso_predictions <- predict(cv_lasso, newx = x_matrix, s = \"lambda.min\")\n\n# Calculate residuals\nresiduals <- y_vector - lasso_predictions\n\n# Calculate standardized residuals\nstandardized_residuals <- residuals / sd(residuals)\n\n# Identify outliers (standardized residuals > 3 or < -3)\noutliers <- which(abs(standardized_residuals) > 3)\n\n# Print outliers\nprint(outliers)\n\n# Plot residual diagnostics\npar(mfrow = c(1, 2))\n\n# Residuals vs Predicted plot\nplot(as.vector(lasso_predictions), as.vector(residuals), \n     main = \"Residuals vs Predicted\",\n     xlab = \"Predicted\", ylab = \"Residuals\")\nabline(h = 0, col = \"red\")\n\n# Histogram of residuals\nhist(residuals, main = \"Histogram of Residuals\", xlab = \"Residuals\")\n\n# Step 1: Remove observations with high residuals\noutlier_indices <- c(50, 83, 161, 225, 410, 601, 852, 946, 962, 1182, 1242, 1358, \n                     1383, 1387, 1442, 1444, 1538, 1638, 1775, 1790, 1794, 1886, \n                     2046, 2134, 2171, 2172, 2233, 2243, 2413, 2797, 2956, 2960, \n                     3050, 3397, 3405)\ncleaned_data_final <- cleaned_data[-outlier_indices, ]\n\n# Step 2: Refit the Lasso model on the filtered data\nx_matrix_final <- model.matrix(log_Total_Affected ~ . -1, data = cleaned_data_final)  # Adjust predictors as necessary\ny_vector_final <- cleaned_data_final$log_Total_Affected\n\n# Fit Lasso model with cross-validation\ncv_lasso_final <- cv.glmnet(x_matrix_final, y_vector_final, alpha = 1)\n\n# Display best lambda\nbest_lambda_final <- cv_lasso_final$lambda.min\ncat(\"Best Lambda (after removing outliers):\", best_lambda_final, \"\\n\")\n\n# Step 3: Reassess residuals for the new model\nlasso_predictions_final <- predict(cv_lasso_final, newx = x_matrix_final, s = \"lambda.min\")\nresiduals_final <- y_vector_final - lasso_predictions_final\n\n# Plot residuals\npar(mfrow = c(1, 2))\nplot(lasso_predictions_final, residuals_final, \n     main = \"Residuals vs Predicted (After Outlier Removal)\", \n     xlab = \"Predicted\", ylab = \"Residuals\")\nabline(h = 0, col = \"red\")\n\nhist(residuals_final, \n     main = \"Histogram of Residuals (After Outlier Removal)\", \n     xlab = \"Residuals\")\n\n# Step 4: Evaluate Model Performance\nfinal_mse <- mean(residuals_final^2)\ncat(\"Final MSE (after removing outliers):\", final_mse, \"\\n\")\n\n\n# Extract non-zero coefficients\nnon_zero_coefficients <- coef(cv_lasso_final, s = \"lambda.min\")\nnon_zero_coefficients <- non_zero_coefficients[non_zero_coefficients != 0]\n\n# Sort coefficients by magnitude\nsorted_coefficients <- sort(non_zero_coefficients, decreasing = TRUE)\n\n# Display feature importance\ncat(\"Feature Importance (Non-Zero Coefficients):\\n\")\nprint(sorted_coefficients)\n\n# Prepare the final report\nfinal_report <- list(\n  Best_Lambda = best_lambda_final,\n  Final_MSE = final_mse,\n  Residual_Plots = \"Attached Residual Plots\",\n  Feature_Importance = sorted_coefficients\n)\n\n# Save results for final reporting\nsave(final_report, file = \"final_model_report.RData\")\n\n# Convert coefficients to a data frame\ncoefficients_df <- as.data.frame(as.matrix(non_zero_coefficients)) # Ensure it's numeric\ncoefficients_df$Feature <- rownames(coefficients_df)\nrownames(coefficients_df) <- NULL\n\n# Rename the coefficient column for clarity\ncolnames(coefficients_df)[1] <- \"Coefficient\"\n\n# Ensure Coefficient is numeric (in case it's being read as a string)\ncoefficients_df$Coefficient <- as.numeric(coefficients_df$Coefficient)\n\n# Sort by absolute coefficient value\ncoefficients_df <- coefficients_df[order(abs(coefficients_df$Coefficient), decreasing = TRUE), ]\n\n# Display top features\nhead(coefficients_df, 20)\n\n# Load ggplot2\nlibrary(ggplot2)\n\n# Plot top 20 features by absolute coefficient value\ncoefficients_df <- coefficients_df[1:20, ] # Keep only the top 20 features\n\n# Create the bar plot\nggplot(coefficients_df, aes(x = reorder(Feature, abs(Coefficient)), y = Coefficient)) +\n  geom_bar(stat = \"identity\", fill = \"steelblue\") +\n  coord_flip() +\n  labs(\n    title = \"Top 20 Features by Absolute Coefficient Value\",\n    x = \"Feature\",\n    y = \"Coefficient\"\n  ) +\n  theme_minimal()\n\n# Load required libraries\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(ggplot2)\nlibrary(plotly)\n\n# Group data by disaster type and OFDA response, and calculate average total affected\ncleaned_data_summary <- cleaned_data_final %>%\n  group_by(`Disaster Type`, OFDA_BHA_Response_Binary) %>%\n  summarise(Average_Total_Affected = mean(`Total Affected`, na.rm = TRUE), .groups = \"drop\")\n\n# Reshape the data to have separate columns for Response (1) and No Response (0)\ncleaned_data_summary_wide <- cleaned_data_summary %>%\n  pivot_wider(names_from = OFDA_BHA_Response_Binary, values_from = Average_Total_Affected, \n              names_prefix = \"Response_\")\n\n# Calculate effectiveness as percentage change\ncleaned_data_summary_wide <- cleaned_data_summary_wide %>%\n  mutate(Effectiveness = (Response_1 - Response_0) / Response_0 * 100)\n\n# View the result\nprint(cleaned_data_summary_wide)\n```\n\n```{r, results='hide', message=FALSE, warning=FALSE,fig.show = 'hide'}\n\n# Base ggplot visualization with improved aesthetics\np <- ggplot(cleaned_data_summary_wide, aes(\n  x = reorder(`Disaster Type`, Effectiveness),\n  y = Effectiveness,\n  fill = `Disaster Type`,\n  text = paste0(\n    \"Disaster Type: \", `Disaster Type`, \"<br>\",\n    \"Effectiveness: \", round(Effectiveness, 2), \"%<br>\",\n    \"Avg Total Affected (No Response): \", round(Response_0, 0), \"<br>\",\n    \"Avg Total Affected (Response): \", round(Response_1, 0)\n  )\n)) +\n  geom_bar(stat = \"identity\", width = 0.7, show.legend = FALSE) +\n  labs(title = \"Effectiveness of OFDA Responses by Disaster Type\",\n       x = \"Disaster Type\", y = \"Effectiveness (%)\") +\n  theme_minimal() +\n  scale_fill_viridis_d(option = \"C\") +  \n  theme(\n    plot.title = element_text(size = 12, face = \"bold\", hjust = 0.5, color = \"black\"),\n    axis.text.x = element_text(angle = 30, hjust = 1, size = 10, color = \"darkgray\"),\n    axis.text.y = element_text(size = 10, color = \"darkgray\"),\n    axis.title = element_text(size = 12, face = \"bold\", color = \"darkblue\")\n  ) +\n  coord_flip()  # Flip coordinates for better readability\n\n# Convert ggplot to an interactive plotly visualization\ninteractive_plot <- ggplotly(p, tooltip = \"text\")\n\n# Add customization for better tooltips and interactivity\ninteractive_plot <- interactive_plot %>%\n  layout(\n    title = list(text = \"<b>Effectiveness of OFDA Responses by Disaster Type</b>\",\n                 font = list(size = 18)),\n    xaxis = list(title = \"Effectiveness (%)\", tickfont = list(size = 10)),\n    yaxis = list(title = \"Disaster Type\", tickfont = list(size = 10)),\n    margin = list(l = 110, r = 50, t = 100, b = 100)\n  )\n\n# Display the interactive plot\ninteractive_plot\n```\n\n\n---\n\n## Research Question 3: Relationship Between Disaster Magnitude and Impact\n```{r, results='hide', message=FALSE, warning=FALSE,fig.show = 'hide'}\ndf = read.csv(\"C:\\\\Users\\\\karti\\\\OneDrive\\\\Desktop\\\\DISASTERS_1988.csv\")\ncolnames(df)\n# Load the dplyr library\nlibrary(dplyr)\ndf <- df %>%\n  rename(Total_Damage_USD = Total_Damage_.US...)\n\nGeo <- df %>%\n  select(Disaster.Type, Country, Region, Magnitude, Magnitude.Scale, Location, Latitude, Longitude, Total.Affected, Total.Deaths, Total_Damage_USD)\n\nhead(Geo)\nsummary(Geo)\n\n# Count of missing values in each column\ncolSums(is.na(Geo))\n\n# Filter out rows with missing values in required columns\nGeo <- Geo %>%\n  filter(!is.na(Latitude) & !is.na(Longitude) & \n           !is.na(Total.Deaths) & !is.na(Total.Affected) & !is.na(Magnitude))\n\n\n\n\n\n# Check the frequency of occurrences per country\ncountry_distribution <- Geo %>%\n  count(Country, sort = TRUE)\n\n# View the top countries\nprint(country_distribution)\n\n# Summarize region-wise distribution\nregion_distribution <- Geo %>%\n  group_by(Region) %>%\n  summarise(\n    Total_Affected = sum(Total.Affected, na.rm = TRUE),\n    Total_Deaths = sum(Total.Deaths, na.rm = TRUE),\n    Total_Damage = sum(Total_Damage_USD, na.rm = TRUE),\n    Count = n()\n  ) %>%\n  arrange(desc(Total_Affected))  # Sort by Total Affected (or change sorting criteria)\n\n# View the summarized data\nprint(region_distribution)\n\n# Summarize frequency and totals per country\ncountry_distribution <- Geo %>%\n  group_by(Country) %>%\n  summarise(\n    Frequency = n(),  # Number of disasters\n    Total_Deaths = sum(Total.Deaths, na.rm = TRUE),\n    Total_Affected = sum(Total.Affected, na.rm = TRUE),\n    Total_Damage = sum(Total_Damage_USD, na.rm = TRUE)\n  ) %>%\n  arrange(desc(Frequency))  # Sort by frequency (or change to Total_Deaths/Total_Damage)\n\n# View the summarized data\nprint(country_distribution, n=150)\n\n\nlibrary(dplyr)\n\n# Calculate the disaster count for each country\nfiltered_geo <- Geo %>%\n  group_by(Country) %>%\n  mutate(Disaster_Count = n()) %>%  # Add a column with disaster count\n  ungroup() %>% \n  filter(Disaster_Count > 4) %>%   # Keep only countries with disaster count >= 4\n  select(-Disaster_Count)           # Optionally, remove the disaster count column\n\n# View the updated dataset\n\n# Summarize frequency and totals per country\ncountry_distribution2 <- filtered_geo %>%\n  group_by(Country) %>%\n  summarise(\n    Frequency = n(),  # Number of disasters\n    Total_Deaths = sum(Total.Deaths, na.rm = TRUE),\n    Total_Affected = sum(Total.Affected, na.rm = TRUE),\n    Total_Damage = sum(Total_Damage_USD, na.rm = TRUE)\n  ) %>%\n  arrange(desc(Frequency))  # Sort by frequency (or change to Total_Deaths/Total_Damage)\n\n# View the summarized data\nprint(country_distribution2, n=100)\n\n\n# Summarize region-wise distribution\nregion_distribution <- filtered_geo %>%\n  group_by(Region) %>%\n  summarise(\n    Total_Affected = sum(Total.Affected, na.rm = TRUE),\n    Total_Deaths = sum(Total.Deaths, na.rm = TRUE),\n    Total_Damage = sum(Total_Damage_USD, na.rm = TRUE),\n    Count = n()\n  ) %>%\n  arrange(desc(Total_Affected))  # Sort by Total Affected (or change sorting criteria)\n\n# View the summarized data\nprint(region_distribution)\n\n# Filter out Oceania from the dataset\nfiltered_geo <- filtered_geo %>%\n  filter(Region != \"Oceania\")\n\nsummary(filtered_geo)\n\nnumeric_data <- filtered_geo %>%\n  select(where(is.numeric))\n\n# Compute the correlation matrix\ncorrelation_matrix <- cor(numeric_data, use = \"complete.obs\")\n\n# View the correlation matrix\nprint(correlation_matrix)\n\nsummary(filtered_geo$Total_Damage_USD)\n\n# Replace missing values with the median\nmedian_damage <- median(filtered_geo$Total_Damage_USD, na.rm = TRUE)\nfiltered_geo <- filtered_geo %>%\n  mutate(\n    Total_Damage_USD = ifelse(is.na(Total_Damage_USD), median_damage, Total_Damage_USD)\n  )\nsummary(filtered_geo$Total_Damage_USD)\n\n# Ensure Disaster.Type column has no leading/trailing spaces and is in lowercase\nfiltered_geo$Disaster.Type <- tolower(trimws(filtered_geo$Disaster.Type))\n\n# Filter out rows where Disaster.Type is \"storm\"\nfiltered_geo <- filtered_geo[filtered_geo$Disaster.Type != \"storm\", ]\n\n# Check the distribution again\ndisaster_type_distribution <- filtered_geo %>%\n  count(Disaster.Type, sort = TRUE)\n\n# View the updated distribution\nprint(disaster_type_distribution)\n```\n\n\n\n```{r, results='hide', message=FALSE, warning=FALSE,fig.show = 'hide'}\n\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(plotly)\n\nvisualize_disaster_types_by_country <- function() {\n  # Summarize the data by Disaster Type and Country\n  disaster_summary <- filtered_geo %>%\n    group_by(Country, Disaster.Type) %>%\n    summarise(\n      Total_Affected = sum(Total.Affected, na.rm = TRUE),\n      Total_Damage_USD = sum(Total_Damage_USD, na.rm = TRUE)\n    ) %>%\n    ungroup() %>%\n    arrange(desc(Total_Affected)) %>%\n    top_n(20, Total_Affected)  # Select top 20 countries by Total Affected\n  \n  # Create the plot\n  plot <- ggplot(disaster_summary, aes(\n    x = reorder(Country, Total_Affected), \n    y = Total_Affected,\n    fill = Disaster.Type,\n    text = paste(\n      \"Country:\", Country,\n      \"<br>Disaster Type:\", Disaster.Type,\n      \"<br>Total Affected:\", Total_Affected,\n      \"<br>Total Damage (USD):\", Total_Damage_USD\n    )\n  )) +\n    geom_bar(stat = \"identity\", position = \"dodge\") +\n    scale_fill_brewer(palette = \"Set2\") +\n    labs(\n      title = \"Top 20 Countries Affected by Disaster Types\",\n      x = \"Country\",\n      y = \"Total Affected\",\n      fill = \"Disaster Type\"\n    ) +\n    theme_minimal() +\n    coord_flip()  # Flip coordinates for better readability\n  \n  # Render the plot with plotly for interactivity\n  ggplotly(plot, tooltip = \"text\")\n}\n\n# Call the function to create and display the visualization\nvisualize_disaster_types_by_country()\n```\n\n\n```{r, results='hide', message=FALSE, warning=FALSE,fig.show = 'hide'}\n\n# Filter data by regions\nasia_data <- filtered_geo %>% filter(Region == \"Asia\")\namericas_data <- filtered_geo %>% filter(Region == \"Americas\")\nafrica_data <- filtered_geo %>% filter(Region == \"Africa\")\neurope_data <- filtered_geo %>% filter(Region == \"Europe\")\n\n\n# Load libraries\nlibrary(dbscan)\nlibrary(sf)\nlibrary(ggplot2)\nlibrary(spatstat)\nlibrary(ggplot2)\nlibrary(plotly)\nlibrary(cluster)\nlibrary(tidyverse)\n\n\n\n\nperform_hdbscan_all_regions <- function() {\n  # Load required libraries\n  if (!requireNamespace(\"dbscan\", quietly = TRUE)) install.packages(\"dbscan\")\n  if (!requireNamespace(\"plotly\", quietly = TRUE)) install.packages(\"plotly\")\n  if (!requireNamespace(\"maps\", quietly = TRUE)) install.packages(\"maps\")\n  \n  library(dbscan)\n  library(plotly)\n  library(maps)\n  library(dplyr)\n  library(ggplot2)\n  \n  # Load world map data\n  world_map <- map_data(\"world\")\n  \n  # Filter and prepare the data for all regions\n  all_regions_data <- filtered_geo %>%\n    select(Region, Magnitude, Total.Affected, Total.Deaths, Total_Damage_USD, Longitude, Latitude, Disaster.Type) %>%\n    drop_na()  # Remove rows with missing values\n  \n  # Scale the numerical data\n  scaled_data <- scale(all_regions_data %>% select(Magnitude, Total.Affected, Total.Deaths, Total_Damage_USD, Longitude, Latitude))\n  \n  # Perform HDBSCAN clustering\n  set.seed(42)  # Ensure reproducibility\n  hdbscan_result <- hdbscan(scaled_data, minPts = 10)  # Adjust minPts as needed\n  \n  # Add cluster labels to the dataset\n  all_regions_data$Cluster <- hdbscan_result$cluster\n  \n  # Remove noise points (Cluster = 0)\n  all_regions_data_filtered <- all_regions_data %>%\n    filter(Cluster != 0)\n  \n  # Create the base map with proper grouping\n  base_map <- ggplot() +\n    geom_polygon(data = world_map, aes(x = long, y = lat, group = group), \n                 fill = \"gray90\", color = \"white\") +\n    coord_fixed(ratio = 1.3) +\n    theme_void()\n  \n  # Overlay clustering visualization\n  cluster_plot <- base_map +\n    geom_point(data = all_regions_data_filtered, aes(x = Longitude, y = Latitude, \n                                                     color = as.factor(Cluster), \n                                                     shape = Disaster.Type, \n                                                     size = Total_Damage_USD / 1e6,\n                                                     text = paste(\n                                                       \"Region:\", Region,\n                                                       \"<br>Cluster:\", Cluster,\n                                                       \"<br>Disaster Type:\", Disaster.Type,\n                                                       \"<br>Magnitude:\", Magnitude,\n                                                       \"<br>Total Affected:\", Total.Affected,\n                                                       \"<br>Total Deaths:\", Total.Deaths,\n                                                       \"<br>Total Damage (USD):\", Total_Damage_USD\n                                                     )), alpha = 0.8) +\n    labs(\n      title = \"HDBSCAN Clustering Analysis with World Map: All Regions\",\n      x = \"Longitude\", y = \"Latitude\", \n      color = \"Cluster\", \n      shape = \"Disaster Type\",\n      size = \"Total Damage (USD, scaled)\"\n    ) +\n    theme_minimal()\n  \n  # Render the plot with plotly for interactivity\n  ggplotly(cluster_plot, tooltip = \"text\")\n}\n\n\n# Call the function to visualize all regions\nperform_hdbscan_all_regions()\n```\n\n\n\n---\n\n\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["styles.css"],"toc":false,"toc-depth":2,"output-file":"code.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.5.57","theme":{"light":"sandstone","dark":"slate"},"page-layout":"full","title":""},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}